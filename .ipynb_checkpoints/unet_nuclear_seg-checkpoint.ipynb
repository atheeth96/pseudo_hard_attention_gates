{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import draw\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split#import spams\n",
    "\n",
    "from skimage.util import pad\n",
    "import skimage\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import math\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.2\n",
    "config.gpu_options.allow_growth=True ##to use gpu as needed\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras : {}\".format(keras.__version__))\n",
    "print(\"tensorflow : {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    \n",
    "    def rot_image(img):\n",
    "        test=img.copy()\n",
    "        rot=imutils.rotate(test,270)\n",
    "        rot=cv2.flip(rot,1)\n",
    "        return rot\n",
    "\n",
    "    def poly2boundry(x,y,img_array):\n",
    "        if len(x)==2 and len(y)==2:\n",
    "            rr,cc=draw.line(x[0],y[0],x[1],y[1])\n",
    "        else:\n",
    "            rr, cc = draw.polygon_perimeter(x, y)\n",
    "        img_array[rr,cc]=[255]\n",
    "        return img_array\n",
    "\n",
    "    def check_in_bounds(x,y,bound):\n",
    "\n",
    "        if x>=bound:\n",
    "            x=bound-1\n",
    "        if y>=bound:\n",
    "            y=bound-1\n",
    "        if x<0:\n",
    "            x=0\n",
    "        if y<0:\n",
    "            y=0\n",
    "        return x,y\n",
    "\n",
    "    \n",
    "    \n",
    "    bi_1_path=\"/datalab/nuclei_seg/binary_images_1\"\n",
    "    if not os.path.exists(bi_1_path):\n",
    "        os.mkdir(bi_1_path)\n",
    "        \n",
    "    bi_2_path=\"/datalab/nuclei_seg/binary_images_2\"\n",
    "    if not os.path.exists(bi_2_path):\n",
    "        os.mkdir(bi_2_path)\n",
    "    png_name=glob.glob('/datalab/nuclei_seg/png_images/*.PNG')\n",
    "    for count,name in enumerate(png_name):\n",
    "        print(\"File {} is {}\".format(count,name))\n",
    "        new_name=name.replace('png_images','binary_images_1')\n",
    "        new_name_2=name.replace('png_images','binary_images_2')\n",
    "        xml_name=name.replace('/datalab/nuclei_seg/png_images/','/datalab/nuclei_seg/Annotations/')\n",
    "        xml_name=xml_name.replace('.PNG','.xml')\n",
    "        tree=ET.parse(xml_name)\n",
    "        root=tree.getroot()\n",
    "        img_test=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "        img_test_2=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "\n",
    "        print(\"The number of regions :{}\".format(len([v.tag for v in root.iter('Vertices')])))\n",
    "        for v in root.iter('Vertices'):\n",
    "            X=[]\n",
    "            Y=[]\n",
    "\n",
    "            for child in v:\n",
    "                x=int(eval(child.attrib['X']))\n",
    "                y=int(eval(child.attrib['Y']))\n",
    "                x,y=check_in_bounds(x,y,1000)\n",
    "\n",
    "\n",
    "\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "\n",
    "\n",
    "            r_nucleus,c_nucleus=draw.polygon(X,Y)\n",
    "            img_test[r_nucleus,c_nucleus]=255\n",
    "            img_test_2=poly2boundry(X,Y,img_test_2)\n",
    "\n",
    "\n",
    "        #img_test.dtype=np.uint8\n",
    "        x,y=np.where(img_test_2==255)\n",
    "        for i,a in enumerate(x):\n",
    "            img_test_2[a-1:a+2,y[i]-1:y[i]+2]=255\n",
    "            #img_test[a-1:a+2,y[i]-1:y[i]+2]=0\n",
    "            img_test_2.dtype='uint8'\n",
    "        \n",
    "        img_test=rot_image(img_test)\n",
    "        img_test_2=rot_image(img_test_2)\n",
    "        cv2.imwrite(new_name,img_test)\n",
    "        cv2.imwrite(new_name_2,img_test_2)\n",
    "\n",
    "    print(\"DONE\")\n",
    "    \n",
    "#create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_list():\n",
    "\n",
    "    img_path='/datalab/nuclei_seg/norm_ideal'\n",
    "    mask_1_path='/datalab/nuclei_seg/binary_images_1'\n",
    "    mask_2_path='/datalab/nuclei_seg/binary_images_2'\n",
    "\n",
    "    img_list=glob.glob('{}/*.PNG'.format(img_path))\n",
    "    b1_list=[x.replace('norm_ideal','binary_images_1') for x in img_list]\n",
    "    b2_list=[x.replace('norm_ideal','binary_images_2') for x in img_list]\n",
    "\n",
    "    for x,y,z in zip(img_list,b1_list,b2_list):\n",
    "\n",
    "        if x.strip('/')[-1]!=y.strip('/')[-1] or x.strip('/')[-1]!=z.strip('/')[-1] or y.strip('/')[-1]!=z.strip('/')[-1]:\n",
    "            print(\"ERROR\")\n",
    "\n",
    "    print(\"NO ERROR\")\n",
    "\n",
    "    \n",
    "\n",
    "check_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(img_path,mask_1_path,mask_2_path,patch_size=128):\n",
    "    bi_1_patches=\"/datalab/nuclei_seg/b1_pat\"\n",
    "    bi_2_patches=\"/datalab/nuclei_seg/b2_pat\"\n",
    "    img_patches=\"/datalab/nuclei_seg/img_pat\"\n",
    "    if not os.path.exists(bi_1_patches):\n",
    "        os.mkdir(bi_1_patches)\n",
    "        print(\"Made {} dir\".format(bi_1_patches))\n",
    "    if not os.path.exists(bi_2_patches):\n",
    "        os.mkdir(bi_2_patches)\n",
    "        print(\"Made {} dir\".format(bi_2_patches))\n",
    "    if not os.path.exists(img_patches):\n",
    "        os.mkdir(img_patches)\n",
    "        print(\"Made {} dir\".format(img_patches))\n",
    "    img_list=glob.glob('{}/*.PNG'.format(img_path))\n",
    "    b1_list=glob.glob('{}/*.PNG'.format(mask_1_path))\n",
    "    b2_list=glob.glob('{}/*.PNG'.format(mask_2_path))\n",
    "    for count,img_path in (enumerate(img_list)):\n",
    "        img=imread(img_path)\n",
    "        b1=imread(b1_list[count])\n",
    "        b2=imread(b1_list[count])\n",
    "        r,c,_=img.shape#1000,1000\n",
    "        \n",
    "        new_r_count=(math.ceil((r-patch_size)/patch_size)+1)#8\n",
    "        new_c_count=(math.ceil((c-patch_size)/patch_size)+1)#8\n",
    "\n",
    "\n",
    "        pad_r1=((new_r_count-1)*patch_size-r+patch_size)//2 #12\n",
    "        pad_r2=((new_r_count-1)*patch_size-r+patch_size)-pad_r1 #12\n",
    "        pad_c1=((new_c_count-1)*patch_size-c+patch_size)//2 #12\n",
    "        pad_c2=((new_c_count-1)*patch_size-c+patch_size)-pad_c1#12\n",
    "\n",
    "\n",
    "\n",
    "        arr_img=np.pad(img, [(pad_r1,pad_r2),(pad_c1,pad_c2),(0,0)], 'constant', constant_values=0)#1024 1024 3\n",
    "        arr_b1=np.pad(b1, [(pad_r1,pad_r2),(pad_c1,pad_c2)], 'constant', constant_values=0)\n",
    "        arr_b2=np.pad(b2, [(pad_r1,pad_r2),(pad_c1,pad_c2)], 'constant', constant_values=0)\n",
    "\n",
    "        \n",
    "        window_shape=(patch_size,patch_size,3)\n",
    "        window_shape_binary=(patch_size,patch_size)\n",
    "        arr_out=skimage.util.view_as_windows(arr_img, window_shape, step=patch_size)\n",
    "        arr_out_binary1=skimage.util.view_as_windows(arr_b1, window_shape_binary, step=patch_size)\n",
    "        arr_out_binary2=skimage.util.view_as_windows(arr_b2, window_shape_binary, step=patch_size)\n",
    "        \n",
    "        x,y=arr_out.shape[:2]\n",
    "        ar2=arr_out.reshape((-1,patch_size,patch_size,3))\n",
    "        ar2_b1=arr_out_binary1.reshape((-1,patch_size,patch_size))\n",
    "        ar2_b2=arr_out_binary2.reshape((-1,patch_size,patch_size))\n",
    "        for i in range(ar2.shape[0]):\n",
    "            \n",
    "            sub_img=ar2[i]\n",
    "            b1_sub_img=ar2_b1[i]\n",
    "            b2_sub_img=ar2_b2[i]\n",
    "            sub_image_path=os.path.join(img_patches,img_path.split('/')[-1].split('.')[0]+\"_{}.png\".format(i))\n",
    "            sub_image_path_b1=os.path.join(bi_1_patches,img_path.split('/')[-1].split('.')[0]+\"_{}.png\".format(i))\n",
    "            sub_image_path_b2=os.path.join(bi_2_patches,img_path.split('/')[-1].split('.')[0]+\"_{}.png\".format(i))\n",
    "            skimage.io.imsave(sub_image_path,sub_img)\n",
    "            skimage.io.imsave(sub_image_path_b1,b1_sub_img)\n",
    "            skimage.io.imsave(sub_image_path_b2,b2_sub_img)\n",
    "            \n",
    "            \n",
    "img_path='/datalab/nuclei_seg/norm_ideal'\n",
    "mask_1_path='/datalab/nuclei_seg/binary_images_1'\n",
    "mask_2_path='/datalab/nuclei_seg/binary_images_2'\n",
    "#create_patches(img_path,mask_1_path,mask_2_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,image_IDs,IMAGE_PATH,MASK_PATH,batch_size=256,sample_wise=False,feature_wise=False,scale=1,\\\n",
    "                 shuffle=True):\n",
    "        print(\"Number of sample : {}\".format(len(image_IDs)))\n",
    "\n",
    "        self.image_IDs = image_IDs\n",
    "        self.IMAGE_PATH=IMAGE_PATH\n",
    "        self.MASK_PATH=MASK_PATH\n",
    "        self.batch_size=batch_size\n",
    "        self.sample_wise=sample_wise\n",
    "        self.feature_wise=feature_wise\n",
    "        self.scale=scale\n",
    "        self.shuffle = shuffle\n",
    "        if self.feature_wise==True:\n",
    "            self.mean_value=np.zeros(3)#np.array([154.55561068,100.37918644,167.97885898])\n",
    "            self.std_value=np.ones(3)#np.array([50.64775462,55.05548022,36.14331006])\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_IDs) / self.batch_size))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        image_ID_temp= self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for ID in image_ID_temp:\n",
    "            x, y = self.__data_generation(ID)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            \n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indices = np.arange(len(self.image_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "\n",
    "    def __data_generation(self, image_ID_temp):\n",
    "        \n",
    "        img_path=os.path.join(self.IMAGE_PATH,self.image_IDs[image_ID_temp])\n",
    "        \n",
    "        mask_name=self.image_IDs[image_ID_temp]#.split('.')[0]+'_mask.jpg'\n",
    "        mask_path_1=os.path.join(self.MASK_PATH[0],mask_name)\n",
    "        mask_path_2=os.path.join(self.MASK_PATH[0],mask_name)\n",
    "        \n",
    "        X = imread(img_path)\n",
    "        #X=resize(X,(1024,1024,3))\n",
    "        \n",
    "        if self.scale!=1 and self.sample_wise==True:\n",
    "            sys.exit(\"Scale and sample_wise, both cannot be implemneted\")\n",
    "        \n",
    "        X=X*self.scale\n",
    "        \n",
    "        if self.sample_wise==True:\n",
    "            X=np.array(X,np.float64)\n",
    "            X[:,:,0]=(X[:,:,0]-np.mean(X[:,:,0]))/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1]-np.mean(X[:,:,1]))/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2]-np.mean(X[:,:,2]))/np.std(X[:,:,2])\n",
    "            \n",
    "        if self.feature_wise==True:\n",
    "            X=np.array(X,np.float64)\n",
    "            X=(X-self.mean_value)/self.std_value\n",
    "        #X=np.expand_dims(X,axis=0)\n",
    "        y1=imread(mask_path_1,as_gray=True)\n",
    "        y2=imread(mask_path_2,as_gray=True)\n",
    "        y=np.expand_dims(y1,axis=2)#np.array([y1])#,y2])\n",
    "        y=np.stack([y1,y2],axis=2)\n",
    "        y=y/255\n",
    "        \n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_PATH='/datalab/nuclei_seg/img_pat'\n",
    "MASK_1_PATH='/datalab/nuclei_seg/b1_pat'\n",
    "MASK_2_PATH='/datalab/nuclei_seg/b2_pat'\n",
    "MASK_PATH=[MASK_1_PATH,MASK_2_PATH]\n",
    "\n",
    "img_list=os.listdir(INPUT_IMAGE_PATH)\n",
    "img_list=[name for name in img_list if '.png' in name ]\n",
    "train_list=img_list[:int(0.8*len(img_list))]\n",
    "val_list=img_list[int(0.8*len(img_list)):]\n",
    "\n",
    "train_generator=UnetDataGenerator(train_list,INPUT_IMAGE_PATH,MASK_PATH,batch_size=32,sample_wise=False,feature_wise=False,scale=1/255,\\\n",
    "                 shuffle=True)\n",
    "val_generator=UnetDataGenerator(val_list,INPUT_IMAGE_PATH,MASK_PATH,batch_size=32,sample_wise=False,feature_wise=False,scale=1/255,\\\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(train_list,INPUT_IMAGE_PATH):    \n",
    "    \n",
    "    x_sum=np.zeros((50,50,3))\n",
    "    for img in tqdm(train_list):\n",
    "        path=os.path.join(INPUT_IMAGE_PATH,img)\n",
    "\n",
    "        x_sum=x_sum+cv2.imread(path)\n",
    "    img_sum=np.sum(x_sum,axis=(0,1))\n",
    "    img_mean=img_sum/(len(train_list)*(50*50))    \n",
    "    x_var=np.zeros((50,50,3))\n",
    "    for img in tqdm(train_list):\n",
    "        path=os.path.join(INPUT_IMAGE_PATH,img)\n",
    "        \n",
    "        x_var+=np.square(cv2.imread(path)-img_mean)\n",
    "    img_var=np.sum(x_var,axis=(0,1))\n",
    "    img_var=img_var/(len(train_list)*(50*50))\n",
    "    img_std=np.sqrt(img_var)\n",
    "    \n",
    "    \n",
    "    #img_mean=np.array([172.74560205,110.46180509,175.13698655])\n",
    "    #img_std=np.array([41.87534665,51.27207546,35.44355781])\n",
    "    print(img_mean,img_std)\n",
    "    return img_mean,img_std\n",
    "\n",
    "if train_generator.feature_wise:\n",
    "    INPUT_IMAGE_PATH='/datalab/nuclei_seg/img_pat'\n",
    "    print('Changing mean and std')\n",
    "    val_generator.feature_wise=True\n",
    "    mean,std=get_mean_std(train_list,INPUT_IMAGE_PATH)\n",
    "    train_generator.mean_value=mean\n",
    "    train_generator.std_value=std\n",
    "    val_generator.mean_value=mean\n",
    "    val_generator.std_value=std\n",
    "else:\n",
    "    print(\"Not Changing mean and std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(index=1):\n",
    "    img,y=train_generator.__getitem__(index)\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    plt.imshow(img[0])\n",
    "    fig2=plt.figure()\n",
    "    print(\"image id \",train_generator.image_IDs[0])\n",
    "    print(\"x shape : {}\\ny shape is : {}\".format(img.shape,y.shape))\n",
    "    \n",
    "    \n",
    "    plt.imshow((y[0][:,:,0]))\n",
    "    #fig3=plt.figure()\n",
    "    #plt.imshow(y[0][:,:,1])\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    \n",
    "    y_true_f_1 = K.flatten(y_true[:,:,:,0])\n",
    "    y_true_f_2 = K.flatten(y_true[:,:,:,1])\n",
    "    \n",
    "    y_pred_f_1 = (K.flatten(y_pred[:,:,:,0]))\n",
    "    y_pred_f_2 = (K.flatten(y_pred[:,:,:,1]))\n",
    "    \n",
    "    intersection_1 = K.sum(y_true_f_1 * y_pred_f_1)\n",
    "    union_1=K.sum(y_true_f_1) + K.sum(y_pred_f_1)\n",
    "    \n",
    "    intersection_2 = K.sum(y_true_f_2 * y_pred_f_2)\n",
    "    union_2=K.sum(y_true_f_2) + K.sum(y_pred_f_2)\n",
    "    \n",
    "    dice_1=(2. * intersection_1) / ( union_1+ K.epsilon())\n",
    "    dice_2=(2. * intersection_2) / ( union_2+ K.epsilon())\n",
    "    \n",
    "    return dice_1*0.6+dice_2*0.4\n",
    "\n",
    "def expend_as(tensor, rep):\n",
    "        my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep})(tensor)\n",
    "        return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape):\n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "    psi = Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    # my_repeat=Lambda(lambda xinput:K.repeat_elements(xinput[0],shape_x[1],axis=1))\n",
    "    # upsample_psi=my_repeat([upsample_psi])\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = multiply([upsample_psi, x])\n",
    "\n",
    "    # print(K.is_keras_tensor(upsample_psi))\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "    result_bn = BatchNormalization()(result)\n",
    "    return result_bn\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm=True):\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 2, (1, 1), strides=(1, 1), padding=\"same\")(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def attn_unet(pretrained_weights = None,output_channels=1,input_size = (128,128,3),act='relu',optimizer='adam'):\n",
    "    inputs = Input(input_size)\n",
    "    assert act in ['relu','leaky_relu']\n",
    "    assert optimizer in ['adam','sgd']\n",
    "    if act=='relu':\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    else:\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    activation='relu'\n",
    "  \n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #bach_size x 256 x 256 x 64\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #bach_size x 128 x 128 x 128\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #bach_size x 64 x 64 x 256\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    \n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    #up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    \n",
    "    \n",
    "    g1=UnetGatingSignal(drop4)\n",
    "    #bach_size x 64 x 64 x 1024\n",
    "    att1=AttnGatingBlock(conv3, g1, 512)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    up7 = Conv2D(256, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))#(conv6))\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    merge7 = concatenate([att1,up7], axis = 3)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    \n",
    "    g2=UnetGatingSignal(conv7)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    att2=AttnGatingBlock(conv2, g2, 256)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    merge8 = concatenate([att2,up8], axis = 3)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    \n",
    "    g3=UnetGatingSignal(conv8)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    att3=AttnGatingBlock(conv1, g3, 128)\n",
    "    #bach_size x 512 x 512 x 64\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    merge9 = concatenate([att3,up9], axis = 3)\n",
    "     #bach_size x 512 x 512 x 128\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(2, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 2\n",
    "    conv10 = Conv2D(output_channels, 2, activation = 'sigmoid',padding='same')(conv9)\n",
    "     #bach_size x 512 x 512 x 1\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "    \n",
    "    if optimizer=='adam':\n",
    "        OPT=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000, amsgrad=False)\n",
    "        print('adam optimizer')\n",
    "    elif optimizer=='sgd':\n",
    "        OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "        print('SGD optimizer')\n",
    "    model.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class CosineWithRestart(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=3,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "class CheckLR(tf.keras.callbacks.Callback):\n",
    "   \n",
    "    def on_epoch_begin(self,epoch,logs):\n",
    "        print(\"LR : {}\".format(K.eval(self.model.optimizer.lr)))\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS=train_generator.__len__()\n",
    "VAL_STEPS=val_generator.__len__()\n",
    "\n",
    "def create_model_dir():\n",
    "    model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    dir_name = os.path.join(os.getcwd(),\"unet_model_{}\".format(model_start_date))\n",
    "\n",
    "    if os.path.exists(dir_name):\n",
    "        print(\"unet_model already exists in {}\".format(dir_name))\n",
    "        return dir_name\n",
    "    else:\n",
    "        os.mkdir(dir_name)\n",
    "        print(\" dir {} made\".format(dir_name))\n",
    "        return(dir_name)\n",
    "    \n",
    "dir_name=create_model_dir()\n",
    "print(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model=attn_unet(input_size=(128,128,3),act='relu',optimizer='sgd')\n",
    "unet_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_on_plateau_cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,mode='min',min_delta=0.001,\\\n",
    "                     patience=4,cooldown=0,min_lr= 1e-12,verbose=1)\n",
    "\n",
    "cosine_annealing_cb=CosineWithRestart(1e-06,0.01,TRAIN_STEPS,lr_decay=1,cycle_length=10,mult_factor=1.5)\n",
    "\n",
    "\n",
    "model_checkpoint_cb=keras.callbacks.ModelCheckpoint('{}/unet_model_optimum_{}.hdf5'.\\\n",
    "                                      format(dir_name,datetime.datetime.now().strftime(\"%Y_%m_%d\")),\\\n",
    "                                      monitor='val_dice_coef', verbose=1, save_best_only=True, \\\n",
    "                                      save_weights_only=True, mode='max', period=1)\n",
    "\n",
    "\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "callbacks=[CheckLR(),model_checkpoint_cb,cosine_annealing_cb]\n",
    "\n",
    "history=unet_model.fit_generator(generator=train_generator,epochs=20,verbose=1,\\\n",
    "                            callbacks=callbacks,\\\n",
    "                                 validation_data=val_generator,use_multiprocessing=True)\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['loss'],label='training_loss')\n",
    "plt.plot(history.history['val_loss'],label='validation_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "fig.savefig('{}/plot_loss_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "f1=history.history['dice_coef']\n",
    "f1_test=history.history['val_dice_coef']\n",
    "\n",
    "fig2=plt.figure()\n",
    "plt.plot(f1,label='training')\n",
    "plt.plot(f1_test,label='testing')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice_score')\n",
    "fig2.savefig('{}/plot_dice_coef_{}.PNG'.format(dir_name,model_start_date))\n",
    "\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "model_json = unet_model.to_json()\n",
    "with open(\"{}/model_{}.json\".format(dir_name,model_start_date), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "unet_model.save_weights(\"{}/unet_model_final_weights_{}.h5\".format(dir_name,model_start_date))\n",
    "print(\"Saved model to disk\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_content=os.listdir()\n",
    "model_list=[s for s in dir_content if 'unet_model_' in s]\n",
    "model_list=sorted(model_list)\n",
    "model_folder=os.path.join(os.getcwd(),model_list[-1])\n",
    "print(\"available models are :\",model_list,\"\\n\",\"selected model is :\", model_folder)\n",
    "model_weights=os.path.join(os.getcwd(),model_folder)+\"/\"+[weights for weights in os.listdir(model_folder) if 'model_final' in weights][-1]\n",
    "model_architecture=glob.glob(os.path.join(os.getcwd(),model_folder)+\"/*.json\")[-1]\n",
    "print(model_weights,model_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(model_architecture, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model_unet = model_from_json(loaded_model_json)\n",
    "#OPT=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "loaded_model_unet.load_weights(model_weights)\n",
    "loaded_model_unet.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef])\n",
    "print(loaded_model_unet.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(img_name,model):\n",
    "    img=imread(img_name)\n",
    "    r,c,_=img.shape#1000,1000\n",
    "\n",
    "    new_r_count=(math.ceil((r-128)/128)+1)#8\n",
    "    new_c_count=(math.ceil((c-128)/128)+1)#8\n",
    "\n",
    "\n",
    "    pad_r1=((new_r_count-1)*128-r+128)//2 #12\n",
    "    pad_r2=((new_r_count-1)*128-r+128)-pad_r1 #12\n",
    "    pad_c1=((new_c_count-1)*128-c+128)//2 #12\n",
    "    pad_c2=((new_c_count-1)*128-c+128)-pad_c1#12\n",
    "\n",
    "\n",
    "    window_shape=(128,128,3)\n",
    "\n",
    "    arr_img=np.pad(img, [(pad_r1,pad_r2),(pad_c1,pad_c2),(0,0)], 'constant', constant_values=0)#1024 1024 3\n",
    "    print(arr_img.shape)\n",
    "\n",
    "\n",
    "    arr_out=skimage.util.view_as_windows(arr_img, window_shape, step=128)\n",
    "    print(arr_out.shape)\n",
    "    x,y=arr_out.shape[:2]\n",
    "    ar2=arr_out.reshape((-1,128,128,3))\n",
    "    print(ar2.shape)\n",
    "    y_pred=model.predict(ar2/255)\n",
    "    #y_pred=y_pred[:,:,:,0]\n",
    "    #y_pred=np.expand_dims(y_pred,axis=3)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    img_temp1=[]\n",
    "    for i in range(x):\n",
    "        img_temp1.append(np.concatenate(y_pred[i*y:(i+1)*y],axis=1))\n",
    "    \n",
    "    img_temp1=np.array(img_temp1,dtype=np.float_)\n",
    "    print(img_temp1.shape)\n",
    "    img_temp1=np.concatenate(img_temp1,axis=0)\n",
    "    print(img_temp1.shape)\n",
    "    img_temp1=img_temp1[pad_r1:img_temp1.shape[0]-pad_r2,pad_c1:img_temp1.shape[1]-pad_c2,:]*255\n",
    "    \n",
    "    return img_temp1\n",
    "    #return y_pred\n",
    "img_path='/datalab/nuclei_seg/norm_ideal'\n",
    "img_list=os.listdir(img_path)\n",
    "img_name=img_path+'/'+img_list[1]\n",
    "y=make_pred(img_name,loaded_model_unet)\n",
    "plt.imshow(y[:,:,0])\n",
    "print(y[:,:,0])\n",
    "cv2.imwrite('asd.jpg',y[:,:,0])\n",
    "\n",
    "print(y.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imread(img_name))\n",
    "print(imread(img_name).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
