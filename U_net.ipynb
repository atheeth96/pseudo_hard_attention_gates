{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import draw\n",
    "from skimage.io import imread,imsave\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util import pad\n",
    "import skimage\n",
    "import time\n",
    "import tqdm\n",
    "import math\n",
    "from DatasetCreator import create_data_vahadane\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from PatchExtractor import PatchExtractor\n",
    "from Dataset import DataSet,Scale,ToTensor,RandomGaussionBlur,RandomMedianBlur\\\n",
    ",RandomHorizontalFlip,RandomRotation,visualize_loader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Models import U_Net, save_model,load_model,init_weights\n",
    "from Metrics import SoftDiceLoss,dice_metric,MultiClassBCE,SoftDiceLoss,DE_loss\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "    \n",
    "import pickle\n",
    "\n",
    "from PredictNucleiMask import whole_img_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_dir='norm_ideal'\n",
    "annotation_dir='Annotations'\n",
    "nucleus_dir='processed_data/nucleus_maps'\n",
    "boundary_dir='processed_data/boundary_maps'\n",
    "\n",
    "h_e_train_dir='processed_data/h_e_train_dir'\n",
    "h_train_dir='processed_data/h_train_dir'\n",
    "nuclei_mask_train_dir='processed_data/nuclei_mask_train_dir'\n",
    "boundary_mask_train_dir='processed_data/boundary_mask_train_dir'\n",
    "\n",
    "h_e_test_dir='processed_data/h_e_test_dir'\n",
    "h_test_dir='processed_data/h_test_dir'\n",
    "nuclei_mask_test_dir='processed_data/nuclei_mask_test_dir'\n",
    "boundary_mask_test_dir='processed_data/boundary_mask_test_dir'\n",
    "\n",
    "h_e_train_patch_dir='processed_data/h_e_train_patch_dir'\n",
    "h_train_patch_dir='processed_data/h_train_patch_dir'\n",
    "nuclei_mask_train_patch_dir='processed_data/nuclei_mask_train_patch_dir'\n",
    "boundary_mask_train_patch_dir='processed_data/boundary_mask_train_patch_dir'\n",
    "\n",
    "h_e_test_patch_dir='processed_data/h_e_test_patch_dir'\n",
    "h_test_patch_dir='processed_data/h_test_patch_dir'\n",
    "nuclei_mask_test_patch_dir='processed_data/nuclei_mask_test_patch_dir'\n",
    "boundary_mask_test_patch_dir='processed_data/boundary_mask_test_patch_dir'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=['TCGA-A7-A13E-01Z-00-DX1.png',\\\n",
    "'TCGA-A7-A13F-01Z-00-DX1.png',\\\n",
    "'TCGA-AR-A1AK-01Z-00-DX1.png',\\\n",
    "'TCGA-AR-A1AS-01Z-00-DX1.png',\\\n",
    "'TCGA-18-5592-01Z-00-DX1.png',\\\n",
    "'TCGA-38-6178-01Z-00-DX1.png',\\\n",
    "'TCGA-49-4488-01Z-00-DX1.png',\\\n",
    "'TCGA-50-5931-01Z-00-DX1.png',\\\n",
    "'TCGA-HE-7130-01Z-00-DX1.png',\\\n",
    "'TCGA-HE-7129-01Z-00-DX1.png',\\\n",
    "'TCGA-B0-5711-01Z-00-DX1.png',\\\n",
    "'TCGA-B0-5698-01Z-00-DX1.png',\\\n",
    "'TCGA-G9-6362-01Z-00-DX1.png',\\\n",
    "'TCGA-G9-6336-01Z-00-DX1.png',\\\n",
    "'TCGA-G9-6363-01Z-00-DX1.png',\\\n",
    "'TCGA-G9-6356-01Z-00-DX1.png']\n",
    "\n",
    "test_list=list(set([x for x in os.listdir(png_dir) if x.split('.')[-1].lower()=='png'])-set(train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN\n",
    "\n",
    "TCGA-A7-A13E-01Z-00-DX1.png<br/>\n",
    "TCGA-A7-A13F-01Z-00-DX1.png<br/>\n",
    "TCGA-AR-A1AK-01Z-00-DX1.png<br/>\n",
    "TCGA-AR-A1AS-01Z-00-DX1.png<br/>\n",
    "TCGA-18-5592-01Z-00-DX1.png<br/>\n",
    "TCGA-38-6178-01Z-00-DX1.png<br/>\n",
    "TCGA-49-4488-01Z-00-DX1.png<br/>\n",
    "TCGA-50-5931-01Z-00-DX1.png<br/>\n",
    "TCGA-HE-7130-01Z-00-DX1.png<br/>\n",
    "TCGA-HE-7129-01Z-00-DX1.png<br/>\n",
    "TCGA-B0-5711-01Z-00-DX1.png<br/>\n",
    "TCGA-B0-5698-01Z-00-DX1.png<br/>\n",
    "TCGA-G9-6362-01Z-00-DX1.png<br/>\n",
    "TCGA-G9-6336-01Z-00-DX1.png<br/>\n",
    "TCGA-G9-6363-01Z-00-DX1.png<br/>\n",
    "TCGA-G9-6356-01Z-00-DX1.png\n",
    "\n",
    "\n",
    "# TEST\n",
    "\n",
    "TCGA-E2-A14V-01Z-00-DX1.png<br/>\n",
    "TCGA-G2-A2EK-01A-02-TSB.png<br/>\n",
    "TCGA-21-5786-01Z-00-DX1.png<br/>\n",
    "TCGA-CH-5767-01Z-00-DX1.png<br/>\n",
    "TCGA-AY-A8YK-01A-01-TS1.png<br/>\n",
    "TCGA-G9-6348-01Z-00-DX1.png<br/>\n",
    "TCGA-B0-5710-01Z-00-DX1.png<br/>\n",
    "TCGA-KB-A93J-01A-01-TS1.png<br/>\n",
    "TCGA-RD-A8N9-01A-01-TS1.png<br/>\n",
    "TCGA-21-5784-01Z-00-DX1.png<br/>\n",
    "TCGA-E2-A1B5-01Z-00-DX1.png<br/>\n",
    "TCGA-DK-A2I6-01A-01-TS1.png<br/>\n",
    "TCGA-NH-A8F7-01A-01-TS1.png<br/>\n",
    "TCGA-HE-7128-01Z-00-DX1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train=4\n",
    "batch_size_test=4\n",
    "                                                \n",
    "train_transform=torchvision.transforms.Compose([RandomGaussionBlur(p=0.4,sigma=0.5,truncate=4,apply_dual=False),\\\n",
    "                                                RandomMedianBlur(p=0.4,disk_rad=1),\\\n",
    "                                                RandomHorizontalFlip(p=0.4),\\\n",
    "                                                Scale(),\\\n",
    "                                                ToTensor()])\n",
    "test_transform=torchvision.transforms.Compose([Scale(),ToTensor()])\n",
    "train_dataset=DataSet(h_e_train_patch_dir,h_train_patch_dir\\\n",
    "                      ,nuclei_mask_train_patch_dir, boundary_mask_train_patch_dir\\\n",
    "                      ,transform=train_transform,attn_gray=True)\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size_train,shuffle=True)\n",
    "test_dataset=DataSet(h_e_test_patch_dir,h_test_patch_dir\\\n",
    "                      ,nuclei_mask_test_patch_dir, boundary_mask_test_patch_dir\\\n",
    "                      ,transform=test_transform,attn_gray=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size_test,shuffle=False)\n",
    "\n",
    "print(train_loader.__len__())\n",
    "print(test_loader.__len__())\n",
    "visualize_loader(test_loader,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=U_Net(img_ch=3,output_ch=1,dropout=0.25)\n",
    "# model=AttnUNet(img_ch=2,output_ch=1,dropout=0.5)\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "BEST_MODEL_PATH=os.path.join(os.getcwd(),'Unet_model_{}'.format(model_start_date))\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    os.mkdir(BEST_MODEL_PATH)\n",
    "    print('{} dir has been made'.format(BEST_MODEL_PATH))\n",
    "print(\"Model's state_dict:\")\n",
    "writer = SummaryWriter('{}/experiment_{}'.format(BEST_MODEL_PATH,1))\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "pretrained=True\n",
    "optimizer_selected='adam'\n",
    "scheduler_type='reduce_on_plateau'\n",
    "batchsize=batch_size_train\n",
    "no_steps=train_dataset.__len__()//batchsize\n",
    "restart_epochs=8\n",
    "num_epochs=50\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#criterion = SoftDiceLoss()#\n",
    "criterion=nn.BCELoss()\n",
    "# criterion=MultiClassBCE(weights=[0.65,0.35])\n",
    "# criterion=DE_loss()\n",
    "\n",
    "history={'train_loss':[],'test_loss':[],'train_dice':[],'test_dice':[]}\n",
    "model = model.to(device)\n",
    "\n",
    "if optimizer_selected=='adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-03, betas=(0.9, 0.98),weight_decay=0.002)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=1e-03, momentum=0.8,nesterov=True)\n",
    "\n",
    "\n",
    "if scheduler_type=='reduce_on_plateau':\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2,\\\n",
    "                                               verbose=True, threshold=0.0001, threshold_mode='rel',\\\n",
    "                                               cooldown=0, min_lr=10e-08, eps=1e-08)\n",
    "    \n",
    "else:\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, restart_epochs*no_steps,\\\n",
    "                                                     eta_min=1e-012, last_epoch=-1)\n",
    "\n",
    "if pretrained:\n",
    "    filename='Unet_model_2020_01_23/model_optim_dice.pth'\n",
    "    load_model(filename,model,optimizer=None,scheduler=None)\n",
    "else:\n",
    "    init_weights(model)\n",
    "\n",
    "best_val=0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"Learning Rate : {}\".format(optimizer.state_dict()['param_groups'][-1]['lr']))\n",
    "    # loop over the dataset multiple times\n",
    "    \n",
    "    run_avg_train_loss=0\n",
    "    run_avg_train_dice=0\n",
    "    \n",
    "   \n",
    "    \n",
    "    run_avg_test_loss=0\n",
    "    run_avg_test_dice=0\n",
    "    \n",
    "    for mode in ['train','eval']:\n",
    "     \n",
    "        if mode == 'train':\n",
    "            \n",
    "            model.train()\n",
    "            loop=tqdm(train_loader)\n",
    "            \n",
    "            for i, sample_batched in (enumerate(loop)):\n",
    "                loop.set_description('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "                \n",
    "                #Clear Gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                h_e_train,nuclei_mask_train= sample_batched['h_e'],sample_batched['nuclei_mask']\n",
    "                \n",
    "                \n",
    "                h_e_train,nuclei_mask_train= \\\n",
    "                h_e_train.to(device, dtype = torch.float),nuclei_mask_train.to(device, dtype = torch.float)\n",
    "\n",
    "                gt_mask_train=nuclei_mask_train\n",
    "\n",
    "                # forward + backward + optimize\n",
    "\n",
    "                outputs = torch.sigmoid(model(h_e_train))\n",
    "\n",
    "                pred_nuclei_train=outputs\n",
    "                \n",
    "\n",
    "                \n",
    "                loss = criterion(outputs, gt_mask_train)\n",
    "                dice_score=dice_metric(pred_nuclei_train,nuclei_mask_train)\n",
    "                run_avg_train_loss=(run_avg_train_loss*(0.9))+loss.detach().item()*0.1\n",
    "                run_avg_train_dice=(run_avg_train_dice*(0.9))+dice_score.detach().item()*0.1\n",
    "               \n",
    "                if (i+1)%100==0:\n",
    "                    \n",
    "                    img_tensor=torch.cat((pred_nuclei_train.detach().cpu(),nuclei_mask_train.detach().cpu()),dim=0)\n",
    "                    \n",
    "                    img_grid2 = torchvision.utils.make_grid(img_tensor,nrow=batch_size_train,padding=10)\n",
    "                    torchvision.utils.save_image\\\n",
    "                    (img_grid2,os.path.join(BEST_MODEL_PATH,\\\n",
    "                                            'train_iter_{}.png'.format(epoch*len(train_loader)+i+1)))\n",
    "                    \n",
    "#                     writer.add_image('TRAIN_ITER_{}'.format(epoch * len(train_loader) + i+1), img_grid2)\n",
    "                    \n",
    "                    \n",
    "                    writer.add_scalar('Training dice score nuclei',\n",
    "                            run_avg_train_dice,\n",
    "                            epoch * len(train_loader) + i+1)\n",
    "                \n",
    "                    writer.add_scalar('Training Loss',\n",
    "                            run_avg_train_loss,\n",
    "                            epoch * len(train_loader) + i+1)\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if scheduler_type!='reduce_on_plateau':\n",
    "                    scheduler.step()\n",
    "                \n",
    "                \n",
    "                loop.set_postfix(loss=run_avg_train_loss,dice_score=run_avg_train_dice)\n",
    "                \n",
    "               \n",
    "            history['train_loss'].append(run_avg_train_loss)\n",
    "            history['train_dice'].append(run_avg_train_dice)\n",
    "            \n",
    "            writer.add_scalar('Train dice { epoch }',\n",
    "                            run_avg_train_dice,\n",
    "                            epoch+1)\n",
    "                \n",
    "            writer.add_scalar('Train loss { epoch }',\n",
    "                    run_avg_train_loss,\n",
    "                    epoch * len(train_loader) + i+1)\n",
    "                \n",
    "                 \n",
    "                    \n",
    "        elif mode =='eval':\n",
    "            #Clear Gradients\n",
    "            optimizer.zero_grad()\n",
    "            samples_test=len(test_loader)\n",
    "            model.eval()\n",
    "            val_loss=0\n",
    "            test_agg=0\n",
    "            for j, test_sample in enumerate(test_loader):\n",
    "\n",
    "#\n",
    "                h_e_test,nuclei_mask_test= test_sample['h_e']\\\n",
    "                ,test_sample['nuclei_mask']\n",
    "\n",
    "                h_e_test,nuclei_mask_test = \\\n",
    "                h_e_test.to(device, dtype = torch.float),nuclei_mask_test.to(device, dtype = torch.float)\n",
    "\n",
    "                gt_mask_test=nuclei_mask_test\n",
    "                test_outputs = torch.sigmoid(model(h_e_test))\n",
    "\n",
    "                pred_nuclei_test=test_outputs\n",
    "                \n",
    "                test_loss = criterion(test_outputs, gt_mask_test)\n",
    "                \n",
    "                test_dice=dice_metric(pred_nuclei_test,nuclei_mask_test)\n",
    "                \n",
    "                run_avg_test_loss=(run_avg_test_loss*(0.9))+test_loss.detach().item()*0.1\n",
    "                run_avg_test_dice=(run_avg_test_dice*(0.9))+test_dice.detach().item()*0.1\n",
    "                \n",
    "               \n",
    "                if (j+1)%50==0:\n",
    "                    \n",
    "    \n",
    "                    img_tensor_test=torch.cat((pred_nuclei_test.detach().cpu(),nuclei_mask_test.detach().cpu()),dim=0)\n",
    "                    \n",
    "                    \n",
    "                    img_grid = torchvision.utils.make_grid(img_tensor_test,nrow=batch_size_test,padding=10)\n",
    "                    torchvision.utils.save_image\\\n",
    "                    (img_grid,os.path.join(BEST_MODEL_PATH,\\\n",
    "                                            'test_iter_{}.png'.format(epoch*len(test_loader)+j+1)))\n",
    "                    \n",
    "#                     writer.add_image('TEST_ITER_{}'.format(epoch * len(test_loader) + j+1), img_grid)\n",
    "                    writer.add_scalar('Testing dice score ',\\\n",
    "                                      run_avg_test_dice,epoch * len(test_loader) + j+1)\n",
    "                    \n",
    "                    writer.add_scalar('Testing Loss',\\\n",
    "                                      run_avg_test_loss,epoch * len(test_loader) + j+1)\n",
    "                \n",
    "            print(\"test_loss: {}\\ntest_dice :{}\"\\\n",
    "                  .format(run_avg_test_loss,run_avg_test_dice))\n",
    "            history['test_loss'].append(run_avg_test_loss)\n",
    "            history['test_dice'].append(run_avg_test_dice)\n",
    "            \n",
    "            writer.add_scalar('Test dice { epoch }',\n",
    "                            run_avg_test_dice,\n",
    "                            epoch+1)\n",
    "                \n",
    "            writer.add_scalar('Test loss { epoch }',\n",
    "                    run_avg_test_loss,\n",
    "                    epoch * len(train_loader) + i+1)\n",
    "            \n",
    "            if run_avg_test_dice>best_val:\n",
    "                best_val=run_avg_test_dice\n",
    "                save_model(model,optimizer,BEST_MODEL_PATH+\\\n",
    "                           '/model_optim.pth',scheduler=scheduler)\n",
    "                print(\"saved model with test dice score: {}\".format(best_val))\n",
    "            if scheduler_type=='reduce_on_plateau':\n",
    "                scheduler.step(run_avg_test_loss)\n",
    "    \n",
    "#             print(\"val_loss {}\".format(val_loss/samples_test))\n",
    "save_model(model,optimizer, BEST_MODEL_PATH+'/model_final.pth',scheduler=scheduler)    \n",
    "\n",
    "with open(\"history.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf Unet_model_2020_01_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
