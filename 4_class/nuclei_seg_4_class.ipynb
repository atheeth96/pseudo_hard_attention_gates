{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import draw\n",
    "from skimage.io import imread\n",
    "from scipy.sparse import csr_matrix,linalg\n",
    "#import spams\n",
    "import time\n",
    "import math\n",
    "from keras.models import Sequential,model_from_json, Input,Model\n",
    "from keras.layers import Conv2D,Dense,MaxPooling2D,Flatten,Dropout,ReLU,Activation\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "import keras\n",
    "from tensorboard import version\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.2\n",
    "config.gpu_options.allow_growth=True ##to use gpu as needed\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 14 15:15:47 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    58W / 300W |    934MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras : 2.2.4\n",
      "tensorflow : 1.14.0\n",
      "tensorboard : 1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras : {}\".format(keras.__version__))\n",
    "print(\"tensorflow : {}\".format(tf.__version__))\n",
    "print(\"tensorboard : {}\".format(version.VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 15:15:57.625965 140399403984704 deprecation_wrapper.py:119] From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0814 15:15:57.628886 140399403984704 deprecation_wrapper.py:119] From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name=glob.glob('/datalab/nuclei_seg/Tissue_images/*.tif')\n",
    "if not os.path.exists('/datalab/nuclei_seg/png_images'):\n",
    "    os.mkdir('/datalab/nuclei_seg/png_images')\n",
    "    print(\"made dir /datalab/nuclei_seg/png_images\")\n",
    "for c,im in tqdm(enumerate(images_name)):\n",
    "    img_name=im\n",
    "    img=cv2.imread(im,1)\n",
    "    \n",
    "    img_name=img_name.replace('Tissue_images','png_images')\n",
    "    img_name=img_name.replace('.tif','.PNG')    \n",
    "    cv2.imwrite(img_name,img)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /datalab/nuclei_seg/4_class/binary_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0 is /datalab/nuclei_seg/png_images/TCGA-G9-6348-01Z-00-DX1.PNG\n",
      "The number of regions :390\n",
      "File 1 is /datalab/nuclei_seg/png_images/TCGA-A7-A13F-01Z-00-DX1.PNG\n",
      "The number of regions :356\n",
      "File 2 is /datalab/nuclei_seg/png_images/TCGA-HE-7129-01Z-00-DX1.PNG\n",
      "The number of regions :1585\n",
      "File 3 is /datalab/nuclei_seg/png_images/TCGA-AR-A1AS-01Z-00-DX1.PNG\n",
      "The number of regions :405\n",
      "File 4 is /datalab/nuclei_seg/png_images/TCGA-HE-7130-01Z-00-DX1.PNG\n",
      "The number of regions :1863\n",
      "File 5 is /datalab/nuclei_seg/png_images/TCGA-B0-5710-01Z-00-DX1.PNG\n",
      "The number of regions :359\n",
      "File 6 is /datalab/nuclei_seg/png_images/TCGA-G9-6356-01Z-00-DX1.PNG\n",
      "The number of regions :442\n",
      "File 7 is /datalab/nuclei_seg/png_images/TCGA-18-5592-01Z-00-DX1.PNG\n",
      "The number of regions :480\n",
      "File 8 is /datalab/nuclei_seg/png_images/TCGA-E2-A1B5-01Z-00-DX1.PNG\n",
      "The number of regions :329\n",
      "File 9 is /datalab/nuclei_seg/png_images/TCGA-49-4488-01Z-00-DX1.PNG\n",
      "The number of regions :557\n",
      "File 10 is /datalab/nuclei_seg/png_images/TCGA-G2-A2EK-01A-02-TSB.PNG\n",
      "The number of regions :401\n",
      "File 11 is /datalab/nuclei_seg/png_images/TCGA-21-5784-01Z-00-DX1.PNG\n",
      "The number of regions :398\n",
      "File 12 is /datalab/nuclei_seg/png_images/TCGA-G9-6362-01Z-00-DX1.PNG\n",
      "The number of regions :472\n",
      "File 13 is /datalab/nuclei_seg/png_images/TCGA-E2-A14V-01Z-00-DX1.PNG\n",
      "The number of regions :378\n",
      "File 14 is /datalab/nuclei_seg/png_images/TCGA-G9-6363-01Z-00-DX1.PNG\n",
      "The number of regions :354\n",
      "File 15 is /datalab/nuclei_seg/png_images/TCGA-38-6178-01Z-00-DX1.PNG\n",
      "The number of regions :424\n",
      "File 16 is /datalab/nuclei_seg/png_images/TCGA-CH-5767-01Z-00-DX1.PNG\n",
      "The number of regions :294\n",
      "File 17 is /datalab/nuclei_seg/png_images/TCGA-G9-6336-01Z-00-DX1.PNG\n",
      "The number of regions :448\n",
      "File 18 is /datalab/nuclei_seg/png_images/TCGA-RD-A8N9-01A-01-TS1.PNG\n",
      "The number of regions :1165\n",
      "File 19 is /datalab/nuclei_seg/png_images/TCGA-NH-A8F7-01A-01-TS1.PNG\n",
      "The number of regions :363\n",
      "File 20 is /datalab/nuclei_seg/png_images/TCGA-AR-A1AK-01Z-00-DX1.PNG\n",
      "The number of regions :433\n",
      "File 21 is /datalab/nuclei_seg/png_images/TCGA-HE-7128-01Z-00-DX1.PNG\n",
      "The number of regions :1076\n",
      "File 22 is /datalab/nuclei_seg/png_images/TCGA-50-5931-01Z-00-DX1.PNG\n",
      "The number of regions :445\n",
      "File 23 is /datalab/nuclei_seg/png_images/TCGA-KB-A93J-01A-01-TS1.PNG\n",
      "The number of regions :1391\n",
      "File 24 is /datalab/nuclei_seg/png_images/TCGA-B0-5698-01Z-00-DX1.PNG\n",
      "The number of regions :357\n",
      "File 25 is /datalab/nuclei_seg/png_images/TCGA-A7-A13E-01Z-00-DX1.PNG\n",
      "The number of regions :314\n",
      "File 26 is /datalab/nuclei_seg/png_images/TCGA-21-5786-01Z-00-DX1.PNG\n",
      "The number of regions :440\n",
      "File 27 is /datalab/nuclei_seg/png_images/TCGA-B0-5711-01Z-00-DX1.PNG\n",
      "The number of regions :342\n",
      "File 28 is /datalab/nuclei_seg/png_images/TCGA-DK-A2I6-01A-01-TS1.PNG\n",
      "The number of regions :342\n",
      "File 29 is /datalab/nuclei_seg/png_images/TCGA-AY-A8YK-01A-01-TS1.PNG\n",
      "The number of regions :363\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "new_name=glob.glob('/datalab/nuclei_seg/png_images/*.PNG')\n",
    "BORDER_VALUE=255\n",
    "NUCLEUS_VALUE=127\n",
    "BG_VALUE=0\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/binary_images'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/binary_images')\n",
    "    print(\"made dir /datalab/nuclei_seg/4_class/binary_images\")\n",
    "    \n",
    "def rot_image(img):\n",
    "    test=img.copy()\n",
    "    rot=imutils.rotate(test,270)\n",
    "    rot=cv2.flip(rot,1)\n",
    "    return rot\n",
    "    \n",
    "def poly2boundry(x,y,img_array):\n",
    "    if len(x)==2 and len(y)==2:\n",
    "        rr,cc=draw.line(x[0],y[0],x[1],y[1])\n",
    "    else:\n",
    "        rr, cc = draw.polygon_perimeter(x, y)\n",
    "    img_array[rr,cc]=[BORDER_VALUE]\n",
    "    return img_array\n",
    "\n",
    "def check_in_bounds(x,y,bound):\n",
    "    \n",
    "    if x>=bound:\n",
    "        x=bound-1\n",
    "    if y>=bound:\n",
    "        y=bound-1\n",
    "    if x<0:\n",
    "        x=0\n",
    "    if y<0:\n",
    "        y=0\n",
    "    return x,y\n",
    "    \n",
    "\n",
    "for count,name in enumerate(new_name):\n",
    "    print(\"File {} is {}\".format(count,name))\n",
    "    new_name=name.replace('/png_images','/4_class/binary_images')\n",
    "    xml_name=name.replace('/datalab/nuclei_seg/png_images/','/datalab/nuclei_seg/Annotations/')\n",
    "    xml_name=xml_name.replace('.PNG','.xml')\n",
    "    tree=ET.parse(xml_name)\n",
    "    root=tree.getroot()\n",
    "    img_test=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "    \n",
    "    print(\"The number of regions :{}\".format(len([v.tag for v in root.iter('Vertices')])))\n",
    "    for v in root.iter('Vertices'):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "\n",
    "        for child in v:\n",
    "            x=int(eval(child.attrib['X']))\n",
    "            y=int(eval(child.attrib['Y']))\n",
    "            x,y=check_in_bounds(x,y,1000)\n",
    "\n",
    "\n",
    "\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "\n",
    "        img_temp=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "        r_nucleus,c_nucleus=draw.polygon(X,Y)\n",
    "        img_test[r_nucleus,c_nucleus]=NUCLEUS_VALUE\n",
    "        img_temp[r_nucleus,c_nucleus]=NUCLEUS_VALUE\n",
    "\n",
    "        img_test=poly2boundry(X,Y,img_test)\n",
    "\n",
    "        area_nuc=len(r_nucleus)\n",
    "        area_eroded=area_nuc\n",
    "        kernel=np.array([np.array([0,1,0]),np.array([1,1,1]),np.array([0,1,0])],np.uint8)\n",
    "\n",
    "\n",
    "        while area_eroded>0.4*area_nuc:\n",
    "            area_eroded=len(np.where(img_temp==NUCLEUS_VALUE)[0])\n",
    "            img_temp=cv2.erode(img_temp,kernel,iterations=1)\n",
    "        \n",
    "        '''\n",
    "        for i,a in enumerate(x):\n",
    "            img_test[a-1:a+2,y[i]-1:y[i]+2]=BORDER_VALUE\n",
    "            img_test.dtype='uint8'\n",
    "        '''\n",
    "            \n",
    "        img_test[np.where(img_temp==NUCLEUS_VALUE)]=63\n",
    "    \n",
    "    img_test.dtype=np.uint8\n",
    "    x,y=np.where(img_test==255)\n",
    "    for i,a in enumerate(x):\n",
    "        img_test[a-1:a+2,y[i]-1:y[i]+2]=BORDER_VALUE\n",
    "    img_test.dtype='uint8'\n",
    "    \n",
    "    cv2.imwrite(new_name,img_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    img_test=rot_image(img_test)\n",
    "    cv2.imwrite(new_name,img_test)\n",
    "    \n",
    "print(\"DONE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snnmf(I):\n",
    "    \n",
    "    I_lab_space = cv2.cvtColor(I, cv2.COLOR_RGB2LAB)\n",
    "    luminosity = I_lab_space[:, :, 0] \n",
    "    mask_lumin = luminosity/ 255 < 0.8\n",
    "    \n",
    "    mask_zero = (I == 0)\n",
    "    I[mask_zero] = 1\n",
    "    \n",
    "    od = (np.maximum(-1 * np.log(I / 255), 1e-6))\n",
    "    od_dl = od[mask_lumin]\n",
    "    od=od.reshape((-1, 3))\n",
    "    od_dl=od_dl.reshape((-1, 3))\n",
    "\n",
    "    dictionary = spams.trainDL(X=od_dl.T, K=2, lambda1=0.1, mode=2,\n",
    "                               modeD=0, posAlpha=True, posD=True, verbose=False).T\n",
    "\n",
    "    # dictionary is 2 x 3\n",
    "    #First one should be Hematoxylin(Bluish-purple)\n",
    "    if dictionary[0, 0] < dictionary[1, 0]:\n",
    "        dictionary = dictionary[[1, 0], :]\n",
    "\n",
    "    dictionary=dictionary / np.linalg.norm(dictionary, axis=1)[:, None]\n",
    "    \n",
    "    sparse=spams.lasso(X=od.T, D=dictionary.T, mode=2, lambda1=0.01, pos=True).toarray().T\n",
    "    \n",
    "    return dictionary,sparse\n",
    "\n",
    "def h_norm(hs,ht):\n",
    "    \n",
    "    ht_rm=np.percentile(ht, 99, axis=0).reshape(1,2)\n",
    "    hs_rm=np.percentile(hs, 99, axis=0).reshape(1,2)\n",
    "    \n",
    "    hs_norm_mat=hs*ht_rm/hs_rm\n",
    "    return hs_norm_mat\n",
    "\n",
    "def save_h(h,w,name,image_dims):\n",
    "    \n",
    "    for i in range(w.shape[1]):\n",
    "\n",
    "        image_OD=255*np.exp(-np.dot(w[:,i].reshape((-1,1)),h[i,:].reshape((1,-1))))\n",
    "        \n",
    "        image_r=image_OD[0,:].reshape((1000,1000))\n",
    "        image_g=image_OD[1,:].reshape((1000,1000))\n",
    "        image_b=image_OD[2,:].reshape((1000,1000))\n",
    "        \n",
    "        image=cv2.merge((image_r,image_g,image_b))\n",
    "        image=image.astype(np.uint8)\n",
    "        \n",
    "        bgr_image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "        gray_image=cv2.cvtColor(bgr_image,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        cv2.imwrite('/datalab/nuclei_seg/{}stain_{}.PNG'.format(name,i+1),bgr_image)\n",
    "        cv2.imwrite('/datalab/nuclei_seg/{}stain_{}_gray.PNG'.format(name,i+1),gray_image)\n",
    "\n",
    "def pad_image(image):\n",
    "    \n",
    "    top,left,right,bottom=[25,25,25,25]\n",
    "    BLACK = [0, 0, 0]\n",
    "    \n",
    "    constant_img = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT,value=[0,0,0])\n",
    "    '''\n",
    "    other parameters include\n",
    "    cv2.BORDER_WRAP\n",
    "    cv2.BORDER_REFLECT\n",
    "    cv2.BORDER_REFLECT_101\n",
    "    cv2.BORDER_CONSTANT,const_val\n",
    "    cv2.BORDER_REPLICATE\n",
    "    \n",
    "    '''\n",
    "    resized_image = cv2.resize(constant_img, (image.shape[0]+50, image.shape[1]+50))\n",
    "    return resized_image\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgt=cv2.imread('/datalab/nuclei_seg/png_images/TCGA-G9-6356-01Z-00-DX1.PNG',1)\n",
    "pattern_norm=re.compile('/datalab/nuclei_seg/png_images/(.*).PNG')\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/norm'):\n",
    "    os.mkdir('/datalab/nuclei_seg/norm')\n",
    "    print(' norm directory has been made')\n",
    "\n",
    "norm_images=glob.glob('/datalab/nuclei_seg/png_images/*.PNG')\n",
    "\n",
    "wt,ht=snnmf(imgt)\n",
    "\n",
    "for i_n,p_image in (enumerate(norm_images)):\n",
    "    norm_file=pattern_norm.search(p_image) \n",
    "    norm_file=norm_file.group(1)\n",
    "\n",
    "    \n",
    "    imgs=cv2.imread(p_image,1)\n",
    "    \n",
    "      \n",
    "    ws,hs=snnmf(imgs)\n",
    "\n",
    "    hs_norm=h_norm(hs,ht)\n",
    "    \n",
    "    od_recon=255 * np.exp(-1 * np.dot(hs_norm, wt))\n",
    "\n",
    "    norm_bgr_image=od_recon.reshape(imgt.shape).astype(np.uint8)\n",
    "    cv2.imwrite('/datalab/nuclei_seg/norm/{}.PNG'.format(norm_file),norm_bgr_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern=re.compile('/datalab/nuclei_seg/norm_ideal/(.*).PNG')\n",
    "norm_images_name=glob.glob('/datalab/nuclei_seg/norm_ideal/*.PNG')\n",
    "train_set=['TCGA-A7-A13E-01Z-00-DX1.PNG','TCGA-A7-A13F-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-AR-A1AK-01Z-00-DX1.PNG','TCGA-AR-A1AS-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-B0-5711-01Z-00-DX1.PNG','TCGA-HE-7128-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-HE-7129-01Z-00-DX1.PNG','TCGA-HE-7130-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-18-5592-01Z-00-DX1.PNG','TCGA-38-6178-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-49-4488-01Z-00-DX1.PNG','TCGA-50-5931-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6336-01Z-00-DX1.PNG','TCGA-G9-6348-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6363-01Z-00-DX1.PNG','TCGA-CH-5767-01Z-00-DX1.PNG']\n",
    "images_name_train=random.sample(train_set,12)\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train')\n",
    "    print(' train directory has been made')\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/nucleus'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/nucleus')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/nucleus dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/border'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/border')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/border dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/background'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/background')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/background dir has been made\")\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/inter'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/inter')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/inter dir has been made\")\n",
    "\n",
    "    \n",
    "for im_num_train,im_name_train in enumerate(images_name_train):\n",
    "    im_name_train=os.path.join('/datalab/nuclei_seg/norm_ideal',im_name_train)\n",
    "    binary_img_name=im_name_train.replace('norm_ideal','4_class/binary_images')\n",
    "    binary_img_train=cv2.imread(binary_img_name,0)\n",
    "\n",
    "    \n",
    "    normal_img=cv2.imread(im_name_train,1) ##CHECK\n",
    "    normal_img=pad_image(normal_img)\n",
    "    file_name=pattern.search(im_name_train)\n",
    "    file_name=file_name.group(1)\n",
    "    nucleus_indices=np.where(binary_img_train==127)\n",
    "    border_indices=np.where(binary_img_train==255)\n",
    "    inter_indices=np.where(binary_img_train==63)\n",
    "    bg_indices=np.where(binary_img_train==0)\n",
    "        \n",
    "    \n",
    "    indices=[nucleus_indices,inter_indices,border_indices,bg_indices]\n",
    "    directory_name=['nucleus','inter','border','background']\n",
    "\n",
    "    \n",
    "\n",
    "    for iter_number,ind_values in enumerate(indices):\n",
    "        \n",
    "        \n",
    "        print(\"Started extracting indices of {}\".format(directory_name[iter_number]))\n",
    "        X_value,Y_value=ind_values\n",
    "        ind=np.random.randint(0,len(X_value)-1,4750)\n",
    "        X_value=X_value[ind]\n",
    "        Y_value=Y_value[ind]\n",
    "  \n",
    "        \n",
    "        dir_name=directory_name[iter_number]\n",
    "\n",
    "        for i,x in enumerate(X_value):\n",
    "            y=Y_value[i]\n",
    "            if x<=25 or y<=25:\n",
    "                continue\n",
    "            y=y+25\n",
    "            x=x+25\n",
    "            sub_image=normal_img[x-25:x+26,y-25:y+26,:]\n",
    "            sub_image_name='/datalab/nuclei_seg/4_class/train/{}/{}_{}.PNG'.format(dir_name,file_name,i)\n",
    "            cv2.imwrite(sub_image_name,sub_image)\n",
    "            print(\"Sub image {}/{} of file {},shaped {} is done\".format(i+1,len(X_value),im_num_train+1,sub_image.shape))\n",
    "\n",
    "            \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_class=np.min([len(os.listdir('/datalab/nuclei_seg/4_class/train/nucleus'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/border'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/background'))])\n",
    "dir_name_train='/datalab/nuclei_seg/4_class/train'\n",
    "train_sub_dir=os.listdir('/datalab/nuclei_seg/4_class/train')\n",
    "for directory in tqdm(train_sub_dir):\n",
    "    directory=os.path.join(dir_name_train,directory)\n",
    "    if len(os.listdir(directory))>minimum_class:\n",
    "        dele_files=np.random.choice(os.listdir(directory),len(os.listdir(directory))-minimum_class)\n",
    "        for files in dele_files:\n",
    "            files=os.path.join(directory,files)\n",
    "            try:\n",
    "                os.remove(files)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir('/datalab/nuclei_seg/4_class/train/nucleus'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/border'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/background'))\\\n",
    "     ,len(os.listdir('/datalab/nuclei_seg/4_class/train/inter')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern=re.compile('/datalab/nuclei_seg/norm_ideal/(.*).PNG')\n",
    "#norm_images_name=glob.glob('/datalab/nuclei_seg/norm_ideal/*.PNG')\n",
    "test_set=['TCGA-E2-A1B5-01Z-00-DX1.PNG','TCGA-E2-A14V-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-B0-5698-01Z-00-DX1.PNG','TCGA-B0-5710-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-21-5786-01Z-00-DX1.PNG','TCGA-21-5784-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6356-01Z-00-DX1.PNG','TCGA-G9-6362-01Z-00-DX1.PNG']\n",
    "images_name_test=random.sample(test_set,4)\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test')\n",
    "    print(' test directory has been made')\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/nucleus'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/nucleus')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/nucleus dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/border'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/border')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/border dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/background'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/background')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/background dir has been made\")\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/inter'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/inter')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/inter dir has been made\")\n",
    "\n",
    "    \n",
    "for im_num_test,im_name_test in enumerate(images_name_test):\n",
    "    im_name_test=os.path.join('/datalab/nuclei_seg/norm_ideal',im_name_test)\n",
    "    binary_img_name=im_name_test.replace('norm_ideal','4_class/binary_images')\n",
    "    binary_img_test=cv2.imread(binary_img_name,0)\n",
    "\n",
    "    \n",
    "    normal_img=cv2.imread(im_name_test,1) ##CHECK\n",
    "    normal_img=pad_image(normal_img)\n",
    "    file_name=pattern.search(im_name_test)\n",
    "    file_name=file_name.group(1)\n",
    "    nucleus_indices=np.where(binary_img_test==127)\n",
    "    border_indices=np.where(binary_img_test==255)\n",
    "    inter_indices=np.where(binary_img_test==63)\n",
    "    bg_indices=np.where(binary_img_test==0)\n",
    "        \n",
    "    \n",
    "    indices=[nucleus_indices,inter_indices,border_indices,bg_indices]\n",
    "    directory_name=['nucleus','inter','border','background']\n",
    "\n",
    "    \n",
    "\n",
    "    for iter_number,ind_values in enumerate(indices):\n",
    "        \n",
    "        \n",
    "        print(\"Started extracting indices of {}\".format(directory_name[iter_number]))\n",
    "        X_value,Y_value=ind_values\n",
    "        ind=np.random.randint(0,len(X_value)-1,4750)\n",
    "        X_value=X_value[ind]\n",
    "        Y_value=Y_value[ind]\n",
    "  \n",
    "        \n",
    "        dir_name=directory_name[iter_number]\n",
    "\n",
    "        for i,x in enumerate(X_value):\n",
    "            y=Y_value[i]\n",
    "            if x<=25 or y<=25:\n",
    "                continue\n",
    "            y=y+25\n",
    "            x=x+25\n",
    "            sub_image=normal_img[x-25:x+26,y-25:y+26,:]\n",
    "            sub_image_name='/datalab/nuclei_seg/4_class/test/{}/{}_{}.PNG'.format(dir_name,file_name,i)\n",
    "            cv2.imwrite(sub_image_name,sub_image)\n",
    "            print(\"Sub image {}/{} of file {},shaped {} is done\".format(i+1,len(X_value),im_num_test+1,sub_image.shape))\n",
    "\n",
    "            \n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "for  direc in tqdm(os.listdir('/datalab/nuclei_seg/4_class/train')):\n",
    "    train_images=glob.glob('/datalab/nuclei_seg/4_calss/train/{}/*.PNG'.format(direc))\n",
    "    \n",
    "    for img in (train_images):\n",
    "        temp=cv2.imread(img,1)\n",
    "        x_train.append(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,PATH,samplewise_center=True, samplewise_std_normalization=True\\\n",
    "                 ,batch_size=256,shuffle=True):\n",
    "        self.PATH=PATH\n",
    "        self.encoding_style={}\n",
    "        self.no_cat=len(os.listdir(self.PATH))\n",
    "        print(\"Number of classes : {}\".format(self.no_cat))\n",
    "        self.sum_image=0\n",
    "        for i,category in enumerate(os.listdir(self.PATH)):\n",
    "            self.encoding_style[category]=i\n",
    "            self.sum_image+=len(os.listdir(os.path.join(self.PATH,category)))\n",
    "        print(\"Total number of images : {}\".format(self.sum_image) )\n",
    "        print(\"INDICES : {}\".format(self.encoding_style))\n",
    "    \n",
    "        \n",
    "        self.samplewise_center=samplewise_center\n",
    "        self.samplewise_std_normalization=samplewise_std_normalization\n",
    "        self.batch_size=batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.sum_image/ self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        batch_image_path= self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X=[]\n",
    "        y=[]\n",
    "        for batch_element_path in batch_image_path:\n",
    "    \n",
    "            X_temp, y_temp = self.__data_generation(batch_element_path)\n",
    "            X.append(X_temp)\n",
    "            y.append(y_temp)\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.image_paths = []\n",
    "        classes_path=[self.PATH+'/'+x for x in os.listdir(self.PATH)]\n",
    "        \n",
    "        for cls in (classes_path):\n",
    "            for image_name in os.listdir(cls):\n",
    "                \n",
    "                image_path=os.path.join(cls,image_name)\n",
    "                self.image_paths.append(image_path)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "                np.random.shuffle(self.image_paths)\n",
    "        \n",
    "\n",
    "    def __data_generation(self, batch_element_path):\n",
    "        class_name=batch_element_path.split('/')[-2]\n",
    "        class_index=self.encoding_style[class_name]\n",
    "    \n",
    "        X = imread(batch_element_path)\n",
    "        if self.samplewise_center==True:\n",
    "            X[:,:,0]=(X[:,:,0]-np.mean(X[:,:,0]))/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1]-np.mean(X[:,:,1]))/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2]-np.mean(X[:,:,2]))/np.std(X[:,:,2])\n",
    "        if self.samplewise_std_normalization==True:\n",
    "            X[:,:,0]=(X[:,:,0])/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1])/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2])/np.std(X[:,:,2])\n",
    "        #X=np.expand_dims(X,axis=0)\n",
    "        y=to_categorical(class_index,self.no_cat)\n",
    "\n",
    "\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD : /datalab/nuclei_seg\n",
      "Number of classes : 4\n",
      "Total number of images : 215288\n",
      "INDICES : {'border': 0, 'nucleus': 1, 'background': 2, 'inter': 3}\n",
      "Number of classes : 4\n",
      "Total number of images : 72739\n",
      "INDICES : {'nucleus': 0, 'background': 1, 'inter': 2, 'border': 3}\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/datalab/nuclei_seg\")\n",
    "print(\"PWD : {}\".format(os.getcwd()))\n",
    "TRAIN_PATH='/datalab/nuclei_seg/4_class/train'\n",
    "TEST_PATH='/datalab/nuclei_seg/4_class/test'\n",
    "\n",
    "#datagen=image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "#datagen.fit(x_train)\n",
    "'''\n",
    "train_generator=datagen.flow_from_directory(directory=TRAIN_PATH,target_size=(51,51),\\\n",
    "                                              color_mode='rgb',batch_size=256,class_mode='categorical',\\\n",
    "                                              shuffle=True,seed=None)\n",
    "test_generator=datagen.flow_from_directory(directory=TEST_PATH,target_size=(51,51),\\\n",
    "                                             color_mode='rgb',batch_size=256,class_mode='categorical',\\\n",
    "                                          shuffle=True,seed=None)\n",
    "\n",
    "'''\n",
    "train_generator=DataGenerator(TRAIN_PATH)\n",
    "test_generator=DataGenerator(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceLR(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n",
    "                 verbose=0, cooldown=0, min_lr=0):\n",
    "\n",
    "        self.monitor = monitor\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('ReduceLROnPlateau '\n",
    "                             'does not support a factor >= 1.0.')\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0  # Cooldown counter.\n",
    "        self.wait = 0\n",
    "        self.prev = 0\n",
    "        self.monitor_op = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \n",
    "        self.monitor_op = lambda a, b: a!=b\n",
    "        self.prev = 0\n",
    "        self.cooldown_counter = 0\n",
    "        self.wait = 0\n",
    "       \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._reset()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "        current = logs.get(self.monitor)\n",
    "        print(\"Wait : {}\".format(self.wait))\n",
    "        if self.wait==0:\n",
    "            self.prev=current\n",
    "       \n",
    "        if self.in_cooldown():\n",
    "            self.cooldown_counter -= 1\n",
    "            self.wait = 0\n",
    "\n",
    "        elif not self.in_cooldown():\n",
    "            if self.monitor_op(current,self.prev):\n",
    "                self.wait=0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = old_lr * self.factor\n",
    "                    new_lr = max(new_lr, self.min_lr)\n",
    "                    K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch {}: ReduceLROnPlateau reducing '\n",
    "                              'learning rate to {}.'.format(epoch + 1, new_lr))\n",
    "                    self.cooldown_counter = self.cooldown\n",
    "                    self.wait = 0\n",
    "\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true,y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    return K.mean(f1)\n",
    "\n",
    "class CheckWeights(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.weights_dif=[]\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"LR : {}\".format(K.eval(self.model.optimizer.lr)))\n",
    "        weights,_biases=model.layers[-1].get_weights()\n",
    "        self.weights_dif.append(weights)\n",
    "        if len(self.weights_dif)==2:\n",
    "            l1=np.subtract(self.weights_dif[1],self.weights_dif[0])\n",
    "            print(\"l1 norm = {},{}\".format(np.max(l1),l1))\n",
    "            self.weights_dif=[]\n",
    "            \n",
    "        \n",
    "class StoreWeights(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,file_path,epoch_check):\n",
    "        self.file_path=file_path\n",
    "        self.epoch_check=epoch_check\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        if epoch==self.epoch_check:\n",
    "            self.model.save_weights(self.file_path, overwrite=True)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestart(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=100,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            if self.cycle_length>= self.steps_per_epoch:\n",
    "                self.cycle_length = self.steps_per_epoch\n",
    "                \n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already exists in model_4class_2019_08_14\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 51, 51, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv_1_layer (Conv2D)        (None, 48, 48, 25)        1225      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 24, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 24, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv_2_layer (Conv2D)        (None, 20, 20, 50)        31300     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv_3_layer (Conv2D)        (None, 5, 5, 80)          144080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 80)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 80)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_1 (Dense)        (None, 1024)              328704    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 1,559,009\n",
      "Trainable params: 1,559,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 16:23:51.600939 140399403984704 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      " 71/841 [=>............................] - ETA: 1:01:23 - loss: 1.3865 - acc: 0.2485 - f1_score: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f9d6dc7d464b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                             ,validation_steps=TEST_STEPS,use_multiprocessing=True,workers=8)\n\u001b[1;32m     54\u001b[0m '''\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m115\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m                            \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstore_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m                            \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def expend_as(tensor, rep):\n",
    "        my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep})(tensor)\n",
    "        return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape):\n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "    psi = Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    # my_repeat=Lambda(lambda xinput:K.repeat_elements(xinput[0],shape_x[1],axis=1))\n",
    "    # upsample_psi=my_repeat([upsample_psi])\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = multiply([upsample_psi, x])\n",
    "\n",
    "    # print(K.is_keras_tensor(upsample_psi))\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "    result_bn = BatchNormalization()(result)\n",
    "    return result_bn\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm=True):\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 2, (1, 1), strides=(1, 1), padding=\"same\")(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def attn_unet(pretrained_weights = None,input_size = (512,512,3),act='relu',optimizer='adam'):\n",
    "    inputs = Input(input_size)\n",
    "    assert act in ['relu','leaky_relu']\n",
    "    assert optimizer in ['adam','sgd']\n",
    "    if act=='relu':\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    else:\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "        conv1 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1=LeakyReLU(alpha=0.5)(conv1)\n",
    "        #bach_size x 512 x 512 x 64\n",
    "    activation='relu'\n",
    "  \n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #bach_size x 256 x 256 x 64\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv2 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #bach_size x 128 x 128 x 128\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv3 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #bach_size x 64 x 64 x 256\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    conv4 = Conv2D(512, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    #bach_size x 64 x 64 x 512\n",
    "    drop4 = Dropout(0.43)(conv4)\n",
    "    \n",
    "    #pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    #up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    #conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    \n",
    "    \n",
    "    g1=UnetGatingSignal(drop4)\n",
    "    #bach_size x 64 x 64 x 1024\n",
    "    att1=AttnGatingBlock(conv3, g1, 512)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    up7 = Conv2D(256, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))#(conv6))\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    merge7 = concatenate([att1,up7], axis = 3)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    conv7 = Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #bach_size x 128 x 128 x 256\n",
    "    \n",
    "    g2=UnetGatingSignal(conv7)\n",
    "    #bach_size x 128 x 128 x 512\n",
    "    att2=AttnGatingBlock(conv2, g2, 256)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    merge8 = concatenate([att2,up8], axis = 3)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    conv8 = Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    #bach_size x 256 x 256 x 128\n",
    "    \n",
    "    g3=UnetGatingSignal(conv8)\n",
    "    #bach_size x 256 x 256 x 256\n",
    "    att3=AttnGatingBlock(conv1, g3, 128)\n",
    "    #bach_size x 512 x 512 x 64\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #bach_size x 512 x 512 x 64\n",
    "    merge9 = concatenate([att3,up9], axis = 3)\n",
    "     #bach_size x 512 x 512 x 128\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 64\n",
    "    conv9 = Conv2D(2, 3, activation = activation, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "     #bach_size x 512 x 512 x 2\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "     #bach_size x 512 x 512 x 1\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "    \n",
    "    if optimizer=='adam':\n",
    "        OPT=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000, amsgrad=False)\n",
    "        print('adam optimizer')\n",
    "    elif optimizer=='sgd':\n",
    "        OPT=keras.optimizers.SGD(lr=0.01,momentum=0.8,nesterov=True)\n",
    "        print('SGD optimizer')\n",
    "    model.compile(optimizer = OPT, loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef,recall,auc])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_content=os.listdir()\n",
    "model_list=[s for s in dir_content if 'model_' in s]\n",
    "model_list=sorted(model_list)\n",
    "model_folder=os.path.join(os.getcwd(),model_list[-2])\n",
    "print(\"available models are :\",model_list,\"\\n\",\"selected model is :\", model_folder)\n",
    "model_weights=os.path.join(os.getcwd(),model_folder)+\"/\"+[weights for weights in os.listdir(model_folder) if 'model_final' in weights][-1]\n",
    "model_architecture=glob.glob(os.path.join(os.getcwd(),model_folder)+\"/*.json\")[-1]\n",
    "print(model_weights,model_architecture)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(model_architecture, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "OPT=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "loaded_model.load_weights(model_weights)\n",
    "loaded_model.compile(loss='categorical_crossentropy',metrics=['accuracy',f1_score],optimizer=OPT)\n",
    "print(loaded_model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(img,model):\n",
    "    padded_img=pad_image(img)\n",
    "    pred_nucleus=[]\n",
    "    pred_border=[]\n",
    "    pred_bg=[]\n",
    "    for X in range(img.shape[0]):\n",
    "        x=X+25\n",
    "        sub_image=[]\n",
    "        for Y in range(img.shape[1]):\n",
    "            y=Y+25\n",
    "            sub_image.append(padded_img[x-25:x+26,y-25:y+26,:])\n",
    "        sub_image=np.array(sub_image)\n",
    "        pred=(model.predict(sub_image,batch_size=32,verbose=1))\n",
    "        pred_nucleus.append(pred[:,2])\n",
    "        pred_border.append(pred[:,1])\n",
    "        pred_bg.append(pred[:,0])\n",
    "        \n",
    "    pred_nucleus=np.array(pred_nucleus)\n",
    "    pred_border=np.array(pred_border)\n",
    "    pred_bg=np.array(pred_bg)\n",
    "    \n",
    "    return pred_nucleus,pred_border,pred_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trai,directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"/atheeth/nuclei_seg/norm_ideal/TCGA-18-5592-01Z-00-DX1.PNG\"\n",
    "img_pred1=cv2.imread(img_path,1)\n",
    "img_pred=img_pred1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus,border,bg=make_predictions(img_pred,model)\n",
    "n=nucleus.copy()\n",
    "n[n<0.5]=0\n",
    "n[n>=0.5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "boundry=border\n",
    "labeled_nucleus,_=scipy.ndimage.measurements.label(n)\n",
    "image_binary=np.zeros(img_pred.shape[0:1])\n",
    "image_color=np.zeros(img_pred.shape)\n",
    "dilation_filter=np.array([[0,1,0],[1,1,1],[0,1,0]],np.uint8)\n",
    "boundry_mean=np.mean(boundry[boundry!=0])\n",
    "max_label=np.amax(labeled_nucleus)\n",
    "current=np.zeros((max_label))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(max_label)):\n",
    "    #print(\"Region {}/{}\".format(i+1,max_label))\n",
    "    temp_binary=np.zeros((1000,1000),np.uint8)\n",
    "    temp_binary[labeled_nucleus==i]=np.uint8(1)\n",
    "    temp_color=np.zeros((1000,1000,3))\n",
    "    temp_color[:,:,0]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color[:,:,1]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color[:,:,2]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color=temp_color.astype(np.uint8)\n",
    "    prev=0\n",
    "    count=0\n",
    "    while current[i]<boundry_mean and count<2:\n",
    "        prev=current[i]\n",
    "        temp1=cv2.dilate(n,dilation_filter,iterations = 1)\n",
    "        temp2=cv2.dilate(temp_color,dilation_filter)\n",
    "        count+=1\n",
    "        diff=(temp1-temp_binary).astype(np.uint8)\n",
    "        temp_multi=np.matmul(diff,boundry)\n",
    "        current[i]=np.mean(temp_multi[temp_multi!=0])\n",
    "    image_binary=image_binary+temp_binary\n",
    "    image_color=image_color+temp_color\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([img_pred[51:102,51:102]])))\n",
    "#print(loaded_model.predict(np.array([img_pred[51:102,51:102]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir=os.listdir(os.path.join(os.getcwd(),'train/background'))\n",
    "print(len(list_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
