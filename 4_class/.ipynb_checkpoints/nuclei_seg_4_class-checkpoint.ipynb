{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import draw\n",
    "from skimage.io import imread\n",
    "from scipy.sparse import csr_matrix,linalg\n",
    "#import spams\n",
    "import time\n",
    "import math\n",
    "from keras.models import Sequential,model_from_json, Input,Model\n",
    "from keras.layers import Conv2D,Dense,MaxPooling2D,Flatten,Dropout,ReLU,Activation\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "#from tensorflow.keras.callbacks import TensorBoard\n",
    "import keras\n",
    "from tensorboard import version\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.2\n",
    "config.gpu_options.allow_growth=True ##to use gpu as needed\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras : {}\".format(keras.__version__))\n",
    "print(\"tensorflow : {}\".format(tf.__version__))\n",
    "print(\"tensorboard : {}\".format(version.VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name=glob.glob('/datalab/nuclei_seg/Tissue_images/*.tif')\n",
    "if not os.path.exists('/datalab/nuclei_seg/png_images'):\n",
    "    os.mkdir('/datalab/nuclei_seg/png_images')\n",
    "    print(\"made dir /datalab/nuclei_seg/png_images\")\n",
    "for c,im in tqdm(enumerate(images_name)):\n",
    "    img_name=im\n",
    "    img=cv2.imread(im,1)\n",
    "    \n",
    "    img_name=img_name.replace('Tissue_images','png_images')\n",
    "    img_name=img_name.replace('.tif','.PNG')    \n",
    "    cv2.imwrite(img_name,img)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /datalab/nuclei_seg/4_class/binary_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name=glob.glob('/datalab/nuclei_seg/png_images/*.PNG')\n",
    "BORDER_VALUE=255\n",
    "NUCLEUS_VALUE=127\n",
    "BG_VALUE=0\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/binary_images'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/binary_images')\n",
    "    print(\"made dir /datalab/nuclei_seg/4_class/binary_images\")\n",
    "    \n",
    "def rot_image(img):\n",
    "    test=img.copy()\n",
    "    rot=imutils.rotate(test,270)\n",
    "    rot=cv2.flip(rot,1)\n",
    "    return rot\n",
    "    \n",
    "def poly2boundry(x,y,img_array):\n",
    "    if len(x)==2 and len(y)==2:\n",
    "        rr,cc=draw.line(x[0],y[0],x[1],y[1])\n",
    "    else:\n",
    "        rr, cc = draw.polygon_perimeter(x, y)\n",
    "    img_array[rr,cc]=[BORDER_VALUE]\n",
    "    return img_array\n",
    "\n",
    "def check_in_bounds(x,y,bound):\n",
    "    \n",
    "    if x>=bound:\n",
    "        x=bound-1\n",
    "    if y>=bound:\n",
    "        y=bound-1\n",
    "    if x<0:\n",
    "        x=0\n",
    "    if y<0:\n",
    "        y=0\n",
    "    return x,y\n",
    "    \n",
    "\n",
    "for count,name in enumerate(new_name):\n",
    "    print(\"File {} is {}\".format(count,name))\n",
    "    new_name=name.replace('/png_images','/4_class/binary_images')\n",
    "    xml_name=name.replace('/datalab/nuclei_seg/png_images/','/datalab/nuclei_seg/Annotations/')\n",
    "    xml_name=xml_name.replace('.PNG','.xml')\n",
    "    tree=ET.parse(xml_name)\n",
    "    root=tree.getroot()\n",
    "    img_test=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "    \n",
    "    print(\"The number of regions :{}\".format(len([v.tag for v in root.iter('Vertices')])))\n",
    "    for v in root.iter('Vertices'):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "\n",
    "        for child in v:\n",
    "            x=int(eval(child.attrib['X']))\n",
    "            y=int(eval(child.attrib['Y']))\n",
    "            x,y=check_in_bounds(x,y,1000)\n",
    "\n",
    "\n",
    "\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "\n",
    "        img_temp=np.zeros(shape=(1000,1000),dtype=np.uint8)\n",
    "        r_nucleus,c_nucleus=draw.polygon(X,Y)\n",
    "        img_test[r_nucleus,c_nucleus]=NUCLEUS_VALUE\n",
    "        img_temp[r_nucleus,c_nucleus]=NUCLEUS_VALUE\n",
    "\n",
    "        img_test=poly2boundry(X,Y,img_test)\n",
    "\n",
    "        area_nuc=len(r_nucleus)\n",
    "        area_eroded=area_nuc\n",
    "        kernel=np.array([np.array([0,1,0]),np.array([1,1,1]),np.array([0,1,0])],np.uint8)\n",
    "\n",
    "\n",
    "        while area_eroded>0.4*area_nuc:\n",
    "            area_eroded=len(np.where(img_temp==NUCLEUS_VALUE)[0])\n",
    "            img_temp=cv2.erode(img_temp,kernel,iterations=1)\n",
    "        \n",
    "        '''\n",
    "        for i,a in enumerate(x):\n",
    "            img_test[a-1:a+2,y[i]-1:y[i]+2]=BORDER_VALUE\n",
    "            img_test.dtype='uint8'\n",
    "        '''\n",
    "            \n",
    "        img_test[np.where(img_temp==NUCLEUS_VALUE)]=63\n",
    "    \n",
    "    img_test.dtype=np.uint8\n",
    "    x,y=np.where(img_test==255)\n",
    "    for i,a in enumerate(x):\n",
    "        img_test[a-1:a+2,y[i]-1:y[i]+2]=BORDER_VALUE\n",
    "    img_test.dtype='uint8'\n",
    "    img_test=rot_image(img_test)\n",
    "    cv2.imwrite(new_name,img_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    img_test=rot_image(img_test)\n",
    "    cv2.imwrite(new_name,img_test)\n",
    "    \n",
    "print(\"DONE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snnmf(I):\n",
    "    \n",
    "    I_lab_space = cv2.cvtColor(I, cv2.COLOR_RGB2LAB)\n",
    "    luminosity = I_lab_space[:, :, 0] \n",
    "    mask_lumin = luminosity/ 255 < 0.8\n",
    "    \n",
    "    mask_zero = (I == 0)\n",
    "    I[mask_zero] = 1\n",
    "    \n",
    "    od = (np.maximum(-1 * np.log(I / 255), 1e-6))\n",
    "    od_dl = od[mask_lumin]\n",
    "    od=od.reshape((-1, 3))\n",
    "    od_dl=od_dl.reshape((-1, 3))\n",
    "\n",
    "    dictionary = spams.trainDL(X=od_dl.T, K=2, lambda1=0.1, mode=2,\n",
    "                               modeD=0, posAlpha=True, posD=True, verbose=False).T\n",
    "\n",
    "    # dictionary is 2 x 3\n",
    "    #First one should be Hematoxylin(Bluish-purple)\n",
    "    if dictionary[0, 0] < dictionary[1, 0]:\n",
    "        dictionary = dictionary[[1, 0], :]\n",
    "\n",
    "    dictionary=dictionary / np.linalg.norm(dictionary, axis=1)[:, None]\n",
    "    \n",
    "    sparse=spams.lasso(X=od.T, D=dictionary.T, mode=2, lambda1=0.01, pos=True).toarray().T\n",
    "    \n",
    "    return dictionary,sparse\n",
    "\n",
    "def h_norm(hs,ht):\n",
    "    \n",
    "    ht_rm=np.percentile(ht, 99, axis=0).reshape(1,2)\n",
    "    hs_rm=np.percentile(hs, 99, axis=0).reshape(1,2)\n",
    "    \n",
    "    hs_norm_mat=hs*ht_rm/hs_rm\n",
    "    return hs_norm_mat\n",
    "\n",
    "def save_h(h,w,name,image_dims):\n",
    "    \n",
    "    for i in range(w.shape[1]):\n",
    "\n",
    "        image_OD=255*np.exp(-np.dot(w[:,i].reshape((-1,1)),h[i,:].reshape((1,-1))))\n",
    "        \n",
    "        image_r=image_OD[0,:].reshape((1000,1000))\n",
    "        image_g=image_OD[1,:].reshape((1000,1000))\n",
    "        image_b=image_OD[2,:].reshape((1000,1000))\n",
    "        \n",
    "        image=cv2.merge((image_r,image_g,image_b))\n",
    "        image=image.astype(np.uint8)\n",
    "        \n",
    "        bgr_image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "        gray_image=cv2.cvtColor(bgr_image,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        cv2.imwrite('/datalab/nuclei_seg/{}stain_{}.PNG'.format(name,i+1),bgr_image)\n",
    "        cv2.imwrite('/datalab/nuclei_seg/{}stain_{}_gray.PNG'.format(name,i+1),gray_image)\n",
    "\n",
    "def pad_image(image):\n",
    "    \n",
    "    top,left,right,bottom=[25,25,25,25]\n",
    "    BLACK = [0, 0, 0]\n",
    "    \n",
    "    constant_img = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT,value=[0,0,0])\n",
    "    '''\n",
    "    other parameters include\n",
    "    cv2.BORDER_WRAP\n",
    "    cv2.BORDER_REFLECT\n",
    "    cv2.BORDER_REFLECT_101\n",
    "    cv2.BORDER_CONSTANT,const_val\n",
    "    cv2.BORDER_REPLICATE\n",
    "    \n",
    "    '''\n",
    "    resized_image = cv2.resize(constant_img, (image.shape[0]+50, image.shape[1]+50))\n",
    "    return resized_image\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgt=cv2.imread('/datalab/nuclei_seg/png_images/TCGA-G9-6356-01Z-00-DX1.PNG',1)\n",
    "pattern_norm=re.compile('/datalab/nuclei_seg/png_images/(.*).PNG')\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/norm'):\n",
    "    os.mkdir('/datalab/nuclei_seg/norm')\n",
    "    print(' norm directory has been made')\n",
    "\n",
    "norm_images=glob.glob('/datalab/nuclei_seg/png_images/*.PNG')\n",
    "\n",
    "wt,ht=snnmf(imgt)\n",
    "\n",
    "for i_n,p_image in (enumerate(norm_images)):\n",
    "    norm_file=pattern_norm.search(p_image) \n",
    "    norm_file=norm_file.group(1)\n",
    "\n",
    "    \n",
    "    imgs=cv2.imread(p_image,1)\n",
    "    \n",
    "      \n",
    "    ws,hs=snnmf(imgs)\n",
    "\n",
    "    hs_norm=h_norm(hs,ht)\n",
    "    \n",
    "    od_recon=255 * np.exp(-1 * np.dot(hs_norm, wt))\n",
    "\n",
    "    norm_bgr_image=od_recon.reshape(imgt.shape).astype(np.uint8)\n",
    "    cv2.imwrite('/datalab/nuclei_seg/norm/{}.PNG'.format(norm_file),norm_bgr_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern=re.compile('/datalab/nuclei_seg/norm_ideal/(.*).PNG')\n",
    "norm_images_name=glob.glob('/datalab/nuclei_seg/norm_ideal/*.PNG')\n",
    "train_set=['TCGA-A7-A13E-01Z-00-DX1.PNG','TCGA-A7-A13F-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-AR-A1AK-01Z-00-DX1.PNG','TCGA-AR-A1AS-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-B0-5711-01Z-00-DX1.PNG','TCGA-HE-7128-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-HE-7129-01Z-00-DX1.PNG','TCGA-HE-7130-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-18-5592-01Z-00-DX1.PNG','TCGA-38-6178-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-49-4488-01Z-00-DX1.PNG','TCGA-50-5931-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6336-01Z-00-DX1.PNG','TCGA-G9-6348-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6363-01Z-00-DX1.PNG','TCGA-CH-5767-01Z-00-DX1.PNG']\n",
    "images_name_train=random.sample(train_set,12)\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train')\n",
    "    print(' train directory has been made')\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/nucleus'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/nucleus')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/nucleus dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/border'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/border')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/border dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/background'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/background')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/background dir has been made\")\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/train/inter'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/train/inter')\n",
    "    print(\"/datalab/nuclei_seg/4_class/train/inter dir has been made\")\n",
    "\n",
    "    \n",
    "for im_num_train,im_name_train in enumerate(images_name_train):\n",
    "    im_name_train=os.path.join('/datalab/nuclei_seg/norm_ideal',im_name_train)\n",
    "    binary_img_name=im_name_train.replace('norm_ideal','4_class/binary_images')\n",
    "    binary_img_train=cv2.imread(binary_img_name,0)\n",
    "\n",
    "    \n",
    "    normal_img=cv2.imread(im_name_train,1) ##CHECK\n",
    "    normal_img=pad_image(normal_img)\n",
    "    file_name=pattern.search(im_name_train)\n",
    "    file_name=file_name.group(1)\n",
    "    nucleus_indices=np.where(binary_img_train==127)\n",
    "    border_indices=np.where(binary_img_train==255)\n",
    "    inter_indices=np.where(binary_img_train==63)\n",
    "    bg_indices=np.where(binary_img_train==0)\n",
    "        \n",
    "    \n",
    "    indices=[nucleus_indices,inter_indices,border_indices,bg_indices]\n",
    "    directory_name=['nucleus','inter','border','background']\n",
    "\n",
    "    \n",
    "\n",
    "    for iter_number,ind_values in enumerate(indices):\n",
    "        \n",
    "        \n",
    "        print(\"Started extracting indices of {}\".format(directory_name[iter_number]))\n",
    "        X_value,Y_value=ind_values\n",
    "        ind=np.random.randint(0,len(X_value)-1,4750)\n",
    "        X_value=X_value[ind]\n",
    "        Y_value=Y_value[ind]\n",
    "  \n",
    "        \n",
    "        dir_name=directory_name[iter_number]\n",
    "\n",
    "        for i,x in enumerate(X_value):\n",
    "            y=Y_value[i]\n",
    "            if x<=25 or y<=25:\n",
    "                continue\n",
    "            y=y+25\n",
    "            x=x+25\n",
    "            sub_image=normal_img[x-25:x+26,y-25:y+26,:]\n",
    "            sub_image_name='/datalab/nuclei_seg/4_class/train/{}/{}_{}.PNG'.format(dir_name,file_name,i)\n",
    "            cv2.imwrite(sub_image_name,sub_image)\n",
    "            print(\"Sub image {}/{} of file {},shaped {} is done\".format(i+1,len(X_value),im_num_train+1,sub_image.shape))\n",
    "\n",
    "            \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_class=np.min([len(os.listdir('/datalab/nuclei_seg/4_class/train/nucleus'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/border'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/background'))])\n",
    "dir_name_train='/datalab/nuclei_seg/4_class/train'\n",
    "train_sub_dir=os.listdir('/datalab/nuclei_seg/4_class/train')\n",
    "for directory in tqdm(train_sub_dir):\n",
    "    directory=os.path.join(dir_name_train,directory)\n",
    "    if len(os.listdir(directory))>minimum_class:\n",
    "        dele_files=np.random.choice(os.listdir(directory),len(os.listdir(directory))-minimum_class)\n",
    "        for files in dele_files:\n",
    "            files=os.path.join(directory,files)\n",
    "            try:\n",
    "                os.remove(files)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir('/datalab/nuclei_seg/4_class/train/nucleus'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/border'))\\\n",
    "                      ,len(os.listdir('/datalab/nuclei_seg/4_class/train/background'))\\\n",
    "     ,len(os.listdir('/datalab/nuclei_seg/4_class/train/inter')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern=re.compile('/datalab/nuclei_seg/norm_ideal/(.*).PNG')\n",
    "#norm_images_name=glob.glob('/datalab/nuclei_seg/norm_ideal/*.PNG')\n",
    "test_set=['TCGA-E2-A1B5-01Z-00-DX1.PNG','TCGA-E2-A14V-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-B0-5698-01Z-00-DX1.PNG','TCGA-B0-5710-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-21-5786-01Z-00-DX1.PNG','TCGA-21-5784-01Z-00-DX1.PNG',\\\n",
    "                  'TCGA-G9-6356-01Z-00-DX1.PNG','TCGA-G9-6362-01Z-00-DX1.PNG']\n",
    "images_name_test=random.sample(test_set,4)\n",
    "\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test')\n",
    "    print(' test directory has been made')\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/nucleus'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/nucleus')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/nucleus dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/border'):\n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/border')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/border dir has been made\")\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/background'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/background')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/background dir has been made\")\n",
    "\n",
    "if not os.path.exists('/datalab/nuclei_seg/4_class/test/inter'):    \n",
    "    os.mkdir('/datalab/nuclei_seg/4_class/test/inter')\n",
    "    print(\"/datalab/nuclei_seg/4_class/test/inter dir has been made\")\n",
    "\n",
    "    \n",
    "for im_num_test,im_name_test in enumerate(images_name_test):\n",
    "    im_name_test=os.path.join('/datalab/nuclei_seg/norm_ideal',im_name_test)\n",
    "    binary_img_name=im_name_test.replace('norm_ideal','4_class/binary_images')\n",
    "    binary_img_test=cv2.imread(binary_img_name,0)\n",
    "\n",
    "    \n",
    "    normal_img=cv2.imread(im_name_test,1) ##CHECK\n",
    "    normal_img=pad_image(normal_img)\n",
    "    file_name=pattern.search(im_name_test)\n",
    "    file_name=file_name.group(1)\n",
    "    nucleus_indices=np.where(binary_img_test==127)\n",
    "    border_indices=np.where(binary_img_test==255)\n",
    "    inter_indices=np.where(binary_img_test==63)\n",
    "    bg_indices=np.where(binary_img_test==0)\n",
    "        \n",
    "    \n",
    "    indices=[nucleus_indices,inter_indices,border_indices,bg_indices]\n",
    "    directory_name=['nucleus','inter','border','background']\n",
    "\n",
    "    \n",
    "\n",
    "    for iter_number,ind_values in enumerate(indices):\n",
    "        \n",
    "        \n",
    "        print(\"Started extracting indices of {}\".format(directory_name[iter_number]))\n",
    "        X_value,Y_value=ind_values\n",
    "        ind=np.random.randint(0,len(X_value)-1,4750)\n",
    "        X_value=X_value[ind]\n",
    "        Y_value=Y_value[ind]\n",
    "  \n",
    "        \n",
    "        dir_name=directory_name[iter_number]\n",
    "\n",
    "        for i,x in enumerate(X_value):\n",
    "            y=Y_value[i]\n",
    "            if x<=25 or y<=25:\n",
    "                continue\n",
    "            y=y+25\n",
    "            x=x+25\n",
    "            sub_image=normal_img[x-25:x+26,y-25:y+26,:]\n",
    "            sub_image_name='/datalab/nuclei_seg/4_class/test/{}/{}_{}.PNG'.format(dir_name,file_name,i)\n",
    "            cv2.imwrite(sub_image_name,sub_image)\n",
    "            print(\"Sub image {}/{} of file {},shaped {} is done\".format(i+1,len(X_value),im_num_test+1,sub_image.shape))\n",
    "\n",
    "            \n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "for  direc in tqdm(os.listdir('/datalab/nuclei_seg/4_class/train')):\n",
    "    train_images=glob.glob('/datalab/nuclei_seg/4_calss/train/{}/*.PNG'.format(direc))\n",
    "    \n",
    "    for img in (train_images):\n",
    "        temp=cv2.imread(img,1)\n",
    "        x_train.append(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,PATH,samplewise_center=True, samplewise_std_normalization=True\\\n",
    "                 ,batch_size=256,shuffle=True):\n",
    "        self.PATH=PATH\n",
    "        self.encoding_style={}\n",
    "        self.no_cat=len(os.listdir(self.PATH))\n",
    "        print(\"Number of classes : {}\".format(self.no_cat))\n",
    "        self.sum_image=0\n",
    "        for i,category in enumerate(os.listdir(self.PATH)):\n",
    "            self.encoding_style[category]=i\n",
    "            self.sum_image+=len(os.listdir(os.path.join(self.PATH,category)))\n",
    "        print(\"Total number of images : {}\".format(self.sum_image) )\n",
    "        print(\"INDICES : {}\".format(self.encoding_style))\n",
    "    \n",
    "        \n",
    "        self.samplewise_center=samplewise_center\n",
    "        self.samplewise_std_normalization=samplewise_std_normalization\n",
    "        self.batch_size=batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.sum_image/ self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        batch_image_path= self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X=[]\n",
    "        y=[]\n",
    "        for batch_element_path in batch_image_path:\n",
    "    \n",
    "            X_temp, y_temp = self.__data_generation(batch_element_path)\n",
    "            X.append(X_temp)\n",
    "            y.append(y_temp)\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.image_paths = []\n",
    "        classes_path=[self.PATH+'/'+x for x in os.listdir(self.PATH)]\n",
    "        \n",
    "        for cls in (classes_path):\n",
    "            for image_name in os.listdir(cls):\n",
    "                \n",
    "                image_path=os.path.join(cls,image_name)\n",
    "                self.image_paths.append(image_path)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "                np.random.shuffle(self.image_paths)\n",
    "        \n",
    "\n",
    "    def __data_generation(self, batch_element_path):\n",
    "        class_name=batch_element_path.split('/')[-2]\n",
    "        class_index=self.encoding_style[class_name]\n",
    "    \n",
    "        X = imread(batch_element_path)\n",
    "        if self.samplewise_center==True:\n",
    "            X[:,:,0]=(X[:,:,0]-np.mean(X[:,:,0]))/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1]-np.mean(X[:,:,1]))/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2]-np.mean(X[:,:,2]))/np.std(X[:,:,2])\n",
    "        if self.samplewise_std_normalization==True:\n",
    "            X[:,:,0]=(X[:,:,0])/np.std(X[:,:,0])\n",
    "            X[:,:,1]=(X[:,:,1])/np.std(X[:,:,1])\n",
    "            X[:,:,2]=(X[:,:,2])/np.std(X[:,:,2])\n",
    "        #X=np.expand_dims(X,axis=0)\n",
    "        y=to_categorical(class_index,self.no_cat)\n",
    "\n",
    "\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD : /datalab/nuclei_seg\n",
      "Number of classes : 4\n",
      "Total number of images : 215288\n",
      "INDICES : {'border': 0, 'nucleus': 1, 'background': 2, 'inter': 3}\n",
      "Number of classes : 4\n",
      "Total number of images : 72739\n",
      "INDICES : {'nucleus': 0, 'background': 1, 'inter': 2, 'border': 3}\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/datalab/nuclei_seg\")\n",
    "print(\"PWD : {}\".format(os.getcwd()))\n",
    "TRAIN_PATH='/datalab/nuclei_seg/4_class/train'\n",
    "TEST_PATH='/datalab/nuclei_seg/4_class/test'\n",
    "'''\n",
    "datagen=image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "#datagen.fit(x_train)\n",
    "\n",
    "train_generator=datagen.flow_from_directory(directory=TRAIN_PATH,target_size=(51,51),\\\n",
    "                                              color_mode='rgb',batch_size=256,class_mode='categorical',\\\n",
    "                                              shuffle=True,seed=None)\n",
    "test_generator=datagen.flow_from_directory(directory=TEST_PATH,target_size=(51,51),\\\n",
    "                                             color_mode='rgb',batch_size=256,class_mode='categorical',\\\n",
    "                                          shuffle=True,seed=None)\n",
    "\n",
    "'''\n",
    "train_generator=DataGenerator(TRAIN_PATH)\n",
    "test_generator=DataGenerator(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceLR(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n",
    "                 verbose=0, cooldown=0, min_lr=0):\n",
    "\n",
    "        self.monitor = monitor\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('ReduceLROnPlateau '\n",
    "                             'does not support a factor >= 1.0.')\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0  # Cooldown counter.\n",
    "        self.wait = 0\n",
    "        self.prev = 0\n",
    "        self.monitor_op = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \n",
    "        self.monitor_op = lambda a, b: a!=b\n",
    "        self.prev = 0\n",
    "        self.cooldown_counter = 0\n",
    "        self.wait = 0\n",
    "       \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._reset()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "        current = logs.get(self.monitor)\n",
    "        print(\"Wait : {}\".format(self.wait))\n",
    "        if self.wait==0:\n",
    "            self.prev=current\n",
    "       \n",
    "        if self.in_cooldown():\n",
    "            self.cooldown_counter -= 1\n",
    "            self.wait = 0\n",
    "\n",
    "        elif not self.in_cooldown():\n",
    "            if self.monitor_op(current,self.prev):\n",
    "                self.wait=0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = old_lr * self.factor\n",
    "                    new_lr = max(new_lr, self.min_lr)\n",
    "                    K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch {}: ReduceLROnPlateau reducing '\n",
    "                              'learning rate to {}.'.format(epoch + 1, new_lr))\n",
    "                    self.cooldown_counter = self.cooldown\n",
    "                    self.wait = 0\n",
    "\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true,y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    return K.mean(f1)\n",
    "\n",
    "class CheckWeights(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.weights_dif=[]\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"LR : {}\".format(K.eval(self.model.optimizer.lr)))\n",
    "        weights,_biases=model.layers[-1].get_weights()\n",
    "        self.weights_dif.append(weights)\n",
    "        if len(self.weights_dif)==2:\n",
    "            l1=np.subtract(self.weights_dif[1],self.weights_dif[0])\n",
    "            print(\"l1 norm = {},{}\".format(np.max(l1),l1))\n",
    "            self.weights_dif=[]\n",
    "            \n",
    "        \n",
    "class StoreWeights(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,file_path,epoch_check):\n",
    "        self.file_path=file_path\n",
    "        self.epoch_check=epoch_check\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        if epoch==self.epoch_check:\n",
    "            self.model.save_weights(self.file_path, overwrite=True)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestart(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=100,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            if self.cycle_length>= self.steps_per_epoch:\n",
    "                self.cycle_length = self.steps_per_epoch\n",
    "                \n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already exists in model_4class_2019_08_09\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 51, 51, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv_1_layer (Conv2D)        (None, 48, 48, 25)        1225      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 24, 24, 25)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 24, 24, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv_2_layer (Conv2D)        (None, 20, 20, 50)        31300     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv_3_layer (Conv2D)        (None, 5, 5, 80)          144080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 2, 2, 80)          0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 2, 2, 80)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_1 (Dense)        (None, 1024)              328704    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 1,559,009\n",
      "Trainable params: 1,559,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/115\n",
      "840/841 [============================>.] - ETA: 1s - loss: 1.3870 - acc: 0.2497 - f1_score: 9.6816e-05"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "TRAIN_STEPS=train_generator.__len__()\n",
    "model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "dir_name = os.path.join(os.getcwd(),\"model_v2_{}\".format(model_start_date))\n",
    "if os.path.exists(dir_name):\n",
    "    print(\"model already exists in {}\".format(\"model_4class_{}\".format(model_start_date)))\n",
    "else:\n",
    "    os.mkdir(dir_name)\n",
    "    print(\" dir {} made\".format(\"model_v2_{}\".format(model_start_date)))\n",
    "\n",
    "model_input=Input((51,51,3),name='input_layer')\n",
    "\n",
    "conv_1=Conv2D(25,kernel_size=(4,4),use_bias=True,kernel_initializer=\\\n",
    "              keras.initializers.RandomNormal(mean=0.0, stddev=0.0001, seed=None),bias_initializer='zeros',\\\n",
    "              strides=(1,1),activation='relu',name='conv_1_layer',input_shape=(51,51,3))(model_input)\n",
    "pool_1=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_1)\n",
    "\n",
    "dp1=Dropout(0.1)(pool_1)\n",
    "\n",
    "\n",
    "conv_2=Conv2D(50,kernel_size=(5,5),kernel_initializer=\\\n",
    "              keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),bias_initializer='zeros',\\\n",
    "              strides=(1,1),activation='relu',name='conv_2_layer')(dp1)\n",
    "pool_2=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_2)\n",
    "dp2=Dropout(0.2)(pool_2)\n",
    "\n",
    "conv_3=Conv2D(80,kernel_size=(6,6),kernel_initializer=\\\n",
    "              keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None),bias_initializer='zeros',\\\n",
    "              strides=(1,1),activation='relu',name='conv_3_layer')(dp2)\n",
    "pool_3=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_3)\n",
    "\n",
    "dp3=Dropout(0.25)(pool_3)\n",
    "flat = Flatten()(dp3)\n",
    "\n",
    "fc1=Dense(1024,name='dense_layer_1',activation='relu',\\\n",
    "          kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\\\n",
    "          ,bias_initializer='zeros',use_bias=True)(flat)\n",
    "dp4=Dropout(0.5)(fc1)\n",
    "fc2=Dense(1024,name='dense_layer_2',\\\n",
    "          kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\\\n",
    "          ,bias_initializer='zeros',activation='relu',use_bias=True)(dp4)\n",
    "dp5=Dropout(0.5)(fc2)\n",
    "model_output=Dense(4,name='output_layer',\\\n",
    "                   kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\\\n",
    "                   ,bias_initializer='zeros',activation='softmax')(dp5)\n",
    "#log_act=Activation(log_softmax,name='SpecialActivation')(model_output)\n",
    "\n",
    "model=Model(inputs=[model_input],outputs=[model_output])\n",
    "\n",
    "store_weights=StoreWeights(file_path='{}/epoch_40_{}.hdf5'.\\\n",
    "                         format(dir_name,datetime.datetime.now().strftime(\"%Y_%m_%d\")),epoch_check=40)\n",
    "check_weights=CheckWeights()\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,mode='min',min_delta=0.01,\\\n",
    "                     patience=4,cooldown=0,min_lr= 1e-12,verbose=1)\n",
    "OPT=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "#OPT=keras.optimizers.SGD(lr=0.01,momentum=0.6,decay=0.00004,nesterov=True)\n",
    "mc_cb=keras.callbacks.ModelCheckpoint('{}/model_optimum_{}.hdf5'.\\\n",
    "                                      format(dir_name,datetime.datetime.now().strftime(\"%Y_%m_%d\")),\\\n",
    "                                      monitor='val_f1_score', verbose=1, save_best_only=True, \\\n",
    "                                      save_weights_only=True, mode='max', period=1)\n",
    "\n",
    "scheduler=CosineWithRestart(1e-06,0.01,TRAIN_STEPS,lr_decay=0.0004,cycle_length=20,mult_factor=2)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPT,metrics=['accuracy',f1_score])\n",
    "\n",
    "history=model.fit_generator(generator=train_generator,epochs=115,verbose=1\\\n",
    "                            ,callbacks=[store_weights,mc_cb,check_weights,reduce_lr],validation_data=test_generator\\\n",
    "                            ,use_multiprocessing=True)\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['loss'],label='training_loss')\n",
    "plt.plot(history.history['val_loss'],label='validation_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "fig.savefig('{}/plot_loss_{}.PNG'.format(dir_name,model_start_date))\n",
    "f1=history.history['f1_score']\n",
    "f1_test=history.history['val_f1_score']\n",
    "fig2=plt.figure()\n",
    "plt.plot(f1,label='training')\n",
    "plt.plot(f1_test,label='testing')\n",
    "plt.legend()\n",
    "print(f1[-1],f1_test[-1])\n",
    "fig2.savefig('{}/plot_f1_macro_{}.PNG'.format(dir_name,model_start_date))\n",
    "model_json = model.to_json()\n",
    "with open(\"{}/model_{}.json\".format(dir_name,model_start_date), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"{}/model_final_weights_{}.h5\".format(dir_name,model_start_date))\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_content=os.listdir()\n",
    "model_list=[s for s in dir_content if 'model_' in s]\n",
    "model_list=sorted(model_list)\n",
    "model_folder=os.path.join(os.getcwd(),model_list[-2])\n",
    "print(\"available models are :\",model_list,\"\\n\",\"selected model is :\", model_folder)\n",
    "model_weights=os.path.join(os.getcwd(),model_folder)+\"/\"+[weights for weights in os.listdir(model_folder) if 'model_final' in weights][-1]\n",
    "model_architecture=glob.glob(os.path.join(os.getcwd(),model_folder)+\"/*.json\")[-1]\n",
    "print(model_weights,model_architecture)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(model_architecture, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "OPT=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "loaded_model.load_weights(model_weights)\n",
    "loaded_model.compile(loss='categorical_crossentropy',metrics=['accuracy',f1_score],optimizer=OPT)\n",
    "print(loaded_model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(img,model):\n",
    "    padded_img=pad_image(img)\n",
    "    pred_nucleus=[]\n",
    "    pred_border=[]\n",
    "    pred_bg=[]\n",
    "    for X in range(img.shape[0]):\n",
    "        x=X+25\n",
    "        sub_image=[]\n",
    "        for Y in range(img.shape[1]):\n",
    "            y=Y+25\n",
    "            sub_image.append(padded_img[x-25:x+26,y-25:y+26,:])\n",
    "        sub_image=np.array(sub_image)\n",
    "        pred=(model.predict(sub_image,batch_size=32,verbose=1))\n",
    "        pred_nucleus.append(pred[:,2])\n",
    "        pred_border.append(pred[:,1])\n",
    "        pred_bg.append(pred[:,0])\n",
    "        \n",
    "    pred_nucleus=np.array(pred_nucleus)\n",
    "    pred_border=np.array(pred_border)\n",
    "    pred_bg=np.array(pred_bg)\n",
    "    \n",
    "    return pred_nucleus,pred_border,pred_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trai,directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"/atheeth/nuclei_seg/norm_ideal/TCGA-18-5592-01Z-00-DX1.PNG\"\n",
    "img_pred1=cv2.imread(img_path,1)\n",
    "img_pred=img_pred1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus,border,bg=make_predictions(img_pred,model)\n",
    "n=nucleus.copy()\n",
    "n[n<0.5]=0\n",
    "n[n>=0.5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "boundry=border\n",
    "labeled_nucleus,_=scipy.ndimage.measurements.label(n)\n",
    "image_binary=np.zeros(img_pred.shape[0:1])\n",
    "image_color=np.zeros(img_pred.shape)\n",
    "dilation_filter=np.array([[0,1,0],[1,1,1],[0,1,0]],np.uint8)\n",
    "boundry_mean=np.mean(boundry[boundry!=0])\n",
    "max_label=np.amax(labeled_nucleus)\n",
    "current=np.zeros((max_label))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(max_label)):\n",
    "    #print(\"Region {}/{}\".format(i+1,max_label))\n",
    "    temp_binary=np.zeros((1000,1000),np.uint8)\n",
    "    temp_binary[labeled_nucleus==i]=np.uint8(1)\n",
    "    temp_color=np.zeros((1000,1000,3))\n",
    "    temp_color[:,:,0]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color[:,:,1]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color[:,:,2]=np.random.randint(0,100)*temp_binary\n",
    "    temp_color=temp_color.astype(np.uint8)\n",
    "    prev=0\n",
    "    count=0\n",
    "    while current[i]<boundry_mean and count<2:\n",
    "        prev=current[i]\n",
    "        temp1=cv2.dilate(n,dilation_filter,iterations = 1)\n",
    "        temp2=cv2.dilate(temp_color,dilation_filter)\n",
    "        count+=1\n",
    "        diff=(temp1-temp_binary).astype(np.uint8)\n",
    "        temp_multi=np.matmul(diff,boundry)\n",
    "        current[i]=np.mean(temp_multi[temp_multi!=0])\n",
    "    image_binary=image_binary+temp_binary\n",
    "    image_color=image_color+temp_color\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([img_pred[51:102,51:102]])))\n",
    "#print(loaded_model.predict(np.array([img_pred[51:102,51:102]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir=os.listdir(os.path.join(os.getcwd(),'train/background'))\n",
    "print(len(list_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
